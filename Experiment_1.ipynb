{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "180w4tPnsOj4j5CPVKvGjNS68P2T5nSep",
      "authorship_tag": "ABX9TyNqpGSBKY40sL3HempW/WeW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/John-D-Boom/geom_virtual_nodes/blob/main/Experiment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4mBZouZNsdo",
        "outputId": "59915c8b-a273-43a7-9b31-1739624a9311"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr 20 17:14:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ge_T_JVWpB9",
        "outputId": "945c2217-5772-461e-9098-48464c31399a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing scatter\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling sparse\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling pytorch geometric\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing rdkit\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling py3Dmol\n"
          ]
        }
      ],
      "source": [
        "#@title [RUN] Install geometric and chem modules\n",
        "import os\n",
        "import torch\n",
        "assert torch.cuda.is_available(), \"WARNING! You are running on a non-GPU instance. For this practical a GPU is highly recommended.\"\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "    print('Installing scatter')\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "    print('Installing sparse')\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "    # print('Installing cluster')\n",
        "    # !pip install -q torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "    print('Installing pytorch geometric')\n",
        "    !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "    print('Installing rdkit')\n",
        "    !pip install -q rdkit-pypi==2021.9.4\n",
        "    print('Installing py3Dmol')\n",
        "    !pip install -q py3Dmol\n",
        "else:\n",
        "    print('already installed. Not repeating')\n",
        "    print('To uninstall: !pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Import python modules\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "from scipy.stats import ortho_group\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import ClusterData\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.datasets import QM9\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_scatter import scatter\n",
        "\n",
        "import rdkit.Chem as Chem\n",
        "from rdkit.Geometry.rdGeometry import Point3D\n",
        "from rdkit.Chem import QED, Crippen, rdMolDescriptors, rdmolops\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "\n",
        "import py3Dmol\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "\n",
        "print(\"All imports succeeded.\")\n",
        "print(\"Python version {}\".format(sys.version))\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA6pkef_XI3p",
        "outputId": "de42d98d-d81b-4897-cd13-2e906dbb2c89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports succeeded.\n",
            "Python version 3.9.16 (main, Dec  7 2022, 01:11:51) \n",
            "[GCC 9.4.0]\n",
            "PyTorch version 2.0.0+cu118\n",
            "PyG version 2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Set random seed for deterministic results\n",
        "\n",
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)\n",
        "print(\"All seeds set.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5PpKUK2XKzJ",
        "outputId": "a6869673-502f-4bf2-f71a-7b8cfb7ce083"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All seeds set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Helper functions for data preparation\n",
        "\n",
        "class SetTarget:\n",
        "    \"\"\"\n",
        "    This transform modifies the labels vector per data sample to only keep \n",
        "    the label for a specific target (there are 19 targets in QM9).\n",
        "\n",
        "    Note: for this practical, we have hardcoded the target to be target #0,\n",
        "    i.e. the electric dipole moment of a drug-like molecule.\n",
        "    (https://en.wikipedia.org/wiki/Electric_dipole_moment)\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        target = 0 # we hardcoded choice of target  \n",
        "        data.y = data.y[:, target]\n",
        "        return data\n",
        "\n",
        "\n",
        "class CompleteGraph:\n",
        "    \"\"\"\n",
        "    This transform adds all pairwise edges into the edge index per data sample, \n",
        "    then removes self loops, i.e. it builds a fully connected or complete graph\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        device = data.edge_index.device\n",
        "\n",
        "        row = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "        col = torch.arange(data.num_nodes, dtype=torch.long, device=device)\n",
        "\n",
        "        row = row.view(-1, 1).repeat(1, data.num_nodes).view(-1)\n",
        "        col = col.repeat(data.num_nodes)\n",
        "        edge_index = torch.stack([row, col], dim=0)\n",
        "\n",
        "        edge_attr = None\n",
        "        if data.edge_attr is not None:\n",
        "            idx = data.edge_index[0] * data.num_nodes + data.edge_index[1]\n",
        "            size = list(data.edge_attr.size())\n",
        "            size[0] = data.num_nodes * data.num_nodes\n",
        "            edge_attr = data.edge_attr.new_zeros(size)\n",
        "            edge_attr[idx] = data.edge_attr\n",
        "\n",
        "        edge_index, edge_attr = remove_self_loops(edge_index, edge_attr)\n",
        "        data.edge_attr = edge_attr\n",
        "        data.edge_index = edge_index\n",
        "\n",
        "        return data\n",
        "\n",
        "print(\"Helper functions loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEqsiW9hXOvz",
        "outputId": "8676a69b-404c-4289-e26e-0e57b78a832c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Helper functions for visualization\n",
        "\n",
        "allowable_atoms = [\n",
        "    \"H\",\n",
        "    \"C\",\n",
        "    \"N\",\n",
        "    \"O\",\n",
        "    \"F\",\n",
        "    \"C\",\n",
        "    \"Cl\",\n",
        "    \"Br\",\n",
        "    \"I\",\n",
        "    \"H\", \n",
        "    \"Unknown\",\n",
        "]\n",
        "\n",
        "def to_atom(t):\n",
        "    try:\n",
        "        return allowable_atoms[int(t.argmax())]\n",
        "    except:\n",
        "        return \"C\"\n",
        "\n",
        "\n",
        "def to_bond_index(t):\n",
        "    t_s = t.squeeze()\n",
        "    return [1, 2, 3, 4][\n",
        "        int(\n",
        "            torch.dot(\n",
        "                t_s,\n",
        "                torch.tensor(\n",
        "                    range(t_s.size()[0]), dtype=torch.float, device=t.device\n",
        "                ),\n",
        "            ).item()\n",
        "        )\n",
        "    ]\n",
        "\n",
        "def to_rdkit(data, device=None):\n",
        "    has_pos = False\n",
        "    node_list = []\n",
        "    for i in range(data.x.size()[0]):\n",
        "        node_list.append(to_atom(data.x[i][:5]))\n",
        "\n",
        "    # create empty editable mol object\n",
        "    mol = Chem.RWMol()\n",
        "    # add atoms to mol and keep track of index\n",
        "    node_to_idx = {}\n",
        "    invalid_idx = set([])\n",
        "    for i in range(len(node_list)):\n",
        "        if node_list[i] == \"Stop\" or node_list[i] == \"H\":\n",
        "            invalid_idx.add(i)\n",
        "            continue\n",
        "        a = Chem.Atom(node_list[i])\n",
        "        molIdx = mol.AddAtom(a)\n",
        "        node_to_idx[i] = molIdx\n",
        "\n",
        "    added_bonds = set([])\n",
        "    for i in range(0, data.edge_index.size()[1]):\n",
        "        ix = data.edge_index[0][i].item()\n",
        "        iy = data.edge_index[1][i].item()\n",
        "        bond = to_bond_index(data.edge_attr[i])  # <font color='red'>TODO</font> fix this\n",
        "        # bond = 1\n",
        "        # add bonds between adjacent atoms\n",
        "\n",
        "        if data.edge_attr[i].sum() == 0:\n",
        "          continue\n",
        "\n",
        "        if (\n",
        "            (str((ix, iy)) in added_bonds)\n",
        "            or (str((iy, ix)) in added_bonds)\n",
        "            or (iy in invalid_idx or ix in invalid_idx)\n",
        "        ):\n",
        "            continue\n",
        "        # add relevant bond type (there are many more of these)\n",
        "\n",
        "        if bond == 0:\n",
        "            continue\n",
        "        elif bond == 1:\n",
        "            bond_type = Chem.rdchem.BondType.SINGLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "        elif bond == 2:\n",
        "            bond_type = Chem.rdchem.BondType.DOUBLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "        elif bond == 3:\n",
        "            bond_type = Chem.rdchem.BondType.TRIPLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "        elif bond == 4:\n",
        "            bond_type = Chem.rdchem.BondType.SINGLE\n",
        "            mol.AddBond(node_to_idx[ix], node_to_idx[iy], bond_type)\n",
        "\n",
        "        added_bonds.add(str((ix, iy)))\n",
        "\n",
        "    if has_pos:\n",
        "        conf = Chem.Conformer(mol.GetNumAtoms())\n",
        "        for i in range(data.pos.size(0)):\n",
        "            if i in invalid_idx:\n",
        "                continue\n",
        "            p = Point3D(\n",
        "                data.pos[i][0].item(),\n",
        "                data.pos[i][1].item(),\n",
        "                data.pos[i][2].item(),\n",
        "            )\n",
        "            conf.SetAtomPosition(node_to_idx[i], p)\n",
        "        conf.SetId(0)\n",
        "        mol.AddConformer(conf)\n",
        "\n",
        "    # Convert RWMol to Mol object\n",
        "    mol = mol.GetMol()\n",
        "    mol_frags = rdmolops.GetMolFrags(mol, asMols=True, sanitizeFrags=False)\n",
        "    largest_mol = max(mol_frags, default=mol, key=lambda m: m.GetNumAtoms())\n",
        "    return largest_mol\n",
        "\n",
        "\n",
        "def MolTo3DView(mol, size=(300, 300), style=\"stick\", surface=False, opacity=0.5):\n",
        "    \"\"\"Draw molecule in 3D\n",
        "    \n",
        "    Args:\n",
        "    ----\n",
        "        mol: rdMol, molecule to show\n",
        "        size: tuple(int, int), canvas size\n",
        "        style: str, type of drawing molecule\n",
        "               style can be 'line', 'stick', 'sphere', 'carton'\n",
        "        surface, bool, display SAS\n",
        "        opacity, float, opacity of surface, range 0.0-1.0\n",
        "    Return:\n",
        "    ----\n",
        "        viewer: py3Dmol.view, a class for constructing embedded 3Dmol.js views in ipython notebooks.\n",
        "    \"\"\"\n",
        "    assert style in ('line', 'stick', 'sphere', 'carton')\n",
        "\n",
        "    mol = Chem.AddHs(mol)\n",
        "    AllChem.EmbedMolecule(mol)\n",
        "    AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
        "    mblock = Chem.MolToMolBlock(mol)\n",
        "    viewer = py3Dmol.view(width=size[0], height=size[1])\n",
        "    viewer.addModel(mblock, 'mol')\n",
        "    viewer.setStyle({style:{}})\n",
        "    if surface:\n",
        "        viewer.addSurface(py3Dmol.SAS, {'opacity': opacity})\n",
        "    viewer.zoomTo()\n",
        "    return viewer\n",
        "\n",
        "def smi2conf(smiles):\n",
        "    '''Convert SMILES to rdkit.Mol with 3D coordinates'''\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol is not None:\n",
        "        mol = Chem.AddHs(mol)\n",
        "        AllChem.EmbedMolecule(mol)\n",
        "        AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
        "        return mol\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "print(\"Helper functions added.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5RXjOw2XVs7",
        "outputId": "f8ae75ac-6745-4998-8d0c-f44f78c2cd54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Helper functions for managing experiments, training, and evaluating models.\n",
        "\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    loss_all = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(data)\n",
        "        loss = F.mse_loss(y_pred, data.y)\n",
        "        loss.backward()\n",
        "        loss_all += loss.item() * data.num_graphs\n",
        "        optimizer.step()\n",
        "    return loss_all / len(train_loader.dataset)\n",
        "\n",
        "\n",
        "def eval(model, loader, device):\n",
        "    model.eval()\n",
        "    error = 0\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(data)\n",
        "            # print(y_pred.device)\n",
        "            # print(std.device)\n",
        "            # Mean Absolute Error using std (computed when preparing data)\n",
        "            error += (y_pred * std - data.y * std).abs().sum().item()\n",
        "    return error / len(loader.dataset)\n",
        "\n",
        "\n",
        "def run_experiment(model, model_name, dataset_name, train_loader, val_loader, test_loader, n_epochs=100):\n",
        "    \n",
        "    print(f\"Running experiment for {model_name}, training on {len(train_loader.dataset)} samples for {n_epochs} epochs.\")\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(\"\\nModel architecture:\")\n",
        "    print(model)\n",
        "    total_param = 0\n",
        "    for param in model.parameters():\n",
        "        total_param += np.prod(list(param.data.size()))\n",
        "    print(f'Total parameters: {total_param}')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Adam optimizer with LR 1e-3\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # LR scheduler which decays LR when validation metric doesn't improve\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.9, patience=5, min_lr=0.00001)\n",
        "    \n",
        "    print(\"\\nStart training:\")\n",
        "    best_val_error = None\n",
        "    perf_per_epoch = [] # Track Test/Val MAE vs. epoch (for plotting)\n",
        "    t = time.time()\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        start_time = time.time()\n",
        "        # Call LR scheduler at start of each epoch\n",
        "        lr = scheduler.optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Train model for one epoch, return avg. training loss\n",
        "        loss = train(model, train_loader, optimizer, device)\n",
        "        \n",
        "        # Evaluate model on validation set\n",
        "        val_error = eval(model, val_loader, device)\n",
        "        \n",
        "        if best_val_error is None or val_error <= best_val_error:\n",
        "            # Evaluate model on test set if validation metric improves\n",
        "            test_error = eval(model, test_loader, device)\n",
        "            best_val_error = val_error\n",
        "        # print(f'Epoch Time: {(time.time()-start_time)/60:.2f} mins')\n",
        "        if epoch % 10 == 0:\n",
        "            # Print and track stats every 10 epochs\n",
        "            print(f'Epoch: {epoch:03d}, LR: {lr:5f}, Loss: {loss:.7f}, '\n",
        "                  f'Val MAE: {val_error:.7f}, Test MAE: {test_error:.7f}')\n",
        "        \n",
        "        scheduler.step(val_error)\n",
        "        perf_per_epoch.append((test_error, val_error, epoch, model_name, dataset_name))\n",
        "        \n",
        "    t = time.time() - t\n",
        "    train_time = t/60\n",
        "    print(f\"\\nDone! Training took {train_time:.2f} mins. Best validation MAE: {best_val_error:.7f}, corresponding test MAE: {test_error:.7f}.\")\n",
        "    \n",
        "    return best_val_error, test_error, train_time, perf_per_epoch"
      ],
      "metadata": {
        "id": "FIVjzXzeUJ0F"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Total number of samples: {len(dataset)}.\")\n",
        "\n",
        "# # Split datasets (in case of using the full dataset)\n",
        "# # test_dataset = dataset[:10000]\n",
        "# # val_dataset = dataset[10000:20000]\n",
        "# # train_dataset = dataset[20000:]\n",
        "\n",
        "# # Split datasets (our 3K subset)\n",
        "# train_dataset = dataset[:1000]\n",
        "# val_dataset = dataset[1000:2000]\n",
        "# test_dataset = dataset[2000:3000]\n",
        "# print(f\"Created dataset splits with {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples.\")\n",
        "\n",
        "# # Create dataloaders with batch size = 32\n",
        "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "izf-3PkGYapE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Helper function for visualizing molecules with virtual nodes\n",
        "import plotly.graph_objects as go\n",
        "from torch_geometric.utils.convert import to_networkx\n",
        "import networkx as nx\n",
        "def plot_molecule_3d(molecule):\n",
        "    G = to_networkx(molecule)\n",
        "    pos = nx.spring_layout(G, dim=3)\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    edge_z = []\n",
        "    for edge in G.edges():\n",
        "        x0, y0, z0 = pos[edge[0]]\n",
        "        x1, y1, z1 = pos[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edge_z.extend([z0, z1, None])\n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_z = []\n",
        "    node_color = []\n",
        "\n",
        "    for node in G.nodes():\n",
        "        x, y, z = pos[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        node_z.append(z)\n",
        "\n",
        "        # Use the first 5 features of the x tensor as a one-hot encoding of the atom identity\n",
        "        if molecule.x.shape[1] == 11:\n",
        "            atom_identity = torch.argmax(molecule.x[node][:5]).item()\n",
        "        elif molecule.x.shape[1] == 12:\n",
        "            atom_identity = torch.argmax(molecule.x[node][:6]).item()\n",
        "        else:\n",
        "            print(\"ERROR: molecule.x has unrecognized dimensions (should be 11 or 12) and so number of atom types cannot be determined\")\n",
        "            return\n",
        "        # Map the atom identity to a color\n",
        "        if atom_identity == 0:\n",
        "            color = 'white' # H\n",
        "        elif atom_identity == 1:\n",
        "            color = 'black' # C\n",
        "        elif atom_identity == 2:\n",
        "            color = 'blue' # N\n",
        "        elif atom_identity == 3:\n",
        "            color = 'red' # O\n",
        "        elif atom_identity == 4:\n",
        "            color = 'purple' # F\n",
        "        elif atom_identity == 5:\n",
        "            color = 'green' # Virtual Node\n",
        "        else:\n",
        "            print('Unrecognized molecule type')\n",
        "            color = 'pink' \n",
        "        node_color.append(color)\n",
        "    node_trace = go.Scatter3d(x=node_x, y=node_y, z=node_z, mode='markers', \n",
        "                                marker=dict(size=8, color=node_color))\n",
        "    edge_trace = go.Scatter3d(x=edge_x, y=edge_y, z=edge_z, mode='lines', \n",
        "                                line=dict(color='black', width=1), hoverinfo='none')\n",
        "    fig = go.Figure(data=[edge_trace, node_trace], layout=go.Layout(\n",
        "        margin=dict(l=0, r=0, b=0, t=0),\n",
        "        scene=dict(xaxis=dict(title='', showticklabels=False, showgrid=False, zeroline=False),\n",
        "                    yaxis=dict(title='', showticklabels=False, showgrid=False, zeroline=False),\n",
        "                    zaxis=dict(title='', showticklabels=False, showgrid=False, zeroline=False)),\n",
        "        showlegend=False))\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "Xi5X0RbFYgtq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Helper Functions to add virtual nodes\n",
        "def add_virtual_one_hot(tensor):\n",
        "    # Get the shape of the input tensor (torch_geometric.data.Data.x)\n",
        "    n, m = tensor.shape\n",
        "    assert m == 11, \"input feature vector should have 11 features, just like atom3d dataset. found {}\".format(m)\n",
        "    new_tensor = torch.zeros((n, 12))\n",
        "    new_tensor[:, :5] = tensor[:, :5]\n",
        "    new_tensor[:, 6:] = tensor[:, 5:]\n",
        "    return new_tensor\n",
        "\n",
        "def pad_edge_attr(tensor):\n",
        "    n, m = tensor.shape\n",
        "    assert m == 4, \"input edge_attr should have 4 features, just like atom3d dataset. found {}\".format(m)\n",
        "\n",
        "    zeros = torch.zeros(n, 1)\n",
        "    # Concatenate the original tensor with the zeros tensor along the second dimension\n",
        "    tensor_nx5 = torch.cat((tensor, zeros), dim=1)\n",
        "    assert tensor_nx5.shape == (n,5), \"edge attr has an unexpected shape: {}\".format(tensor_nx5.shape)\n",
        "    return tensor_nx5\n",
        "\n",
        "\n",
        "def add_virtual_node(graph_in: torch_geometric.data.Data, nodes_to_connect: list):\n",
        "    graph = copy.copy(graph_in)\n",
        "    #Node will be appended to the end of the array\n",
        "    virtual_index = len(graph.x) \n",
        "\n",
        "    #Pad the features with an extra one hot column in the 6th position to \n",
        "    #indicate the virtual node\n",
        "    #Only do this if no virtual nodes have been added before\n",
        "    if graph.x.shape[1] == 11:\n",
        "        graph.x = add_virtual_one_hot(graph.x)\n",
        "\n",
        "    #Add node to x features\n",
        "    new_node_x = torch.zeros((1,12))\n",
        "    new_node_x[0,5] = 1\n",
        "    graph.x = torch.cat([graph.x, new_node_x], dim=0)\n",
        "    assert graph.x.shape[0] == virtual_index + 1, print(graph.x.shape)\n",
        "\n",
        "    #Add edges connecting the node to nodes_to_connect\n",
        "    new_edges = [[], []]\n",
        "    for node in nodes_to_connect:\n",
        "        assert node >= 0\n",
        "        assert node < len(graph.x-1) #node must have been possible in orig graph\n",
        "\n",
        "        #Add edge in both directions\n",
        "        new_edges[0].append(virtual_index)\n",
        "        new_edges[1].append(node)\n",
        "\n",
        "        new_edges[0].append(node)\n",
        "        new_edges[1].append(virtual_index)\n",
        "\n",
        "    #Add edges connecting the virtual node to all other virtual nodes\n",
        "    # 1. get list of the nodes with virtual_node identifier\n",
        "    # 2. Fully connect it\n",
        "\n",
        "    virtual_indices = torch.nonzero(graph.x[:-1, 5] == 1, as_tuple=False) #:-1 to not include itself\n",
        "    for idx in virtual_indices:\n",
        "        node = int(idx[0])\n",
        "        assert node >= 0\n",
        "        assert node < len(graph.x-1)\n",
        "        assert node != virtual_index\n",
        "        new_edges[0].append(virtual_index)\n",
        "        new_edges[1].append(node)\n",
        "\n",
        "        new_edges[0].append(node)\n",
        "        new_edges[1].append(virtual_index)\n",
        "\n",
        "    graph.edge_index = torch.cat([graph.edge_index, torch.tensor(new_edges)], dim = 1)\n",
        "\n",
        "\n",
        "    #Add a position to node based on arithmetic mean of positions\n",
        "    virtual_pos = torch.zeros((1,3))\n",
        "    for node in nodes_to_connect:\n",
        "        assert node >= 0\n",
        "        assert node < len(graph.x-1) #node must have been possible in orig graph\n",
        "        virtual_pos = virtual_pos + graph.pos[node]\n",
        "    virtual_pos = virtual_pos / (virtual_index-1)\n",
        "    graph.pos = torch.cat([graph.pos, virtual_pos])\n",
        "    \n",
        "    #update z just cuz\n",
        "    graph.z = torch.cat([graph.z, torch.tensor([0])])\n",
        "\n",
        "    #update edge_attributes to include a one-hot encoding space \n",
        "    #for a virtual edge if there isn't space already\n",
        "    if graph.edge_attr.shape[1] == 4:\n",
        "        graph.edge_attr = pad_edge_attr(graph.edge_attr)\n",
        "\n",
        "    new_edge_attr = torch.zeros((len(new_edges[0])), 5)\n",
        "    new_edge_attr[:, 4] = 1 #should be last space\n",
        "    graph.edge_attr = torch.cat([graph.edge_attr, new_edge_attr], dim = 0)\n",
        "        \n",
        "    return graph"
      ],
      "metadata": {
        "id": "0loW3PaAaRFQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Helper Function to use to assign nodes to a virtual node with METIS Clustering\n",
        "def get_clusters(data:torch_geometric.data.Data, num_clusters: int):\n",
        "\n",
        "    cluster_data = ClusterData(data, num_parts=num_clusters, recursive=False, log=False)\n",
        "    \n",
        "    clusters = {} #key: cluster_number | value: list of nodes in that cluster\n",
        "    for i, cluster in enumerate(cluster_data):\n",
        "        clusters[i] = []\n",
        "        for node_pos in cluster.pos:\n",
        "            node_index = int(torch.nonzero(torch.eq(node_pos, data.pos).all(dim=1))[0][0])\n",
        "            clusters[i].append(node_index)\n",
        "\n",
        "    return clusters"
      ],
      "metadata": {
        "id": "YV4XxsyJaaQ1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Rotation/Translation Equivariance Unit Test\n",
        "\n",
        "def random_orthogonal_matrix(dim=3):\n",
        "  \"\"\"Helper function to build a random orthogonal matrix of shape (dim, dim)\n",
        "  \"\"\"\n",
        "  Q = torch.tensor(ortho_group.rvs(dim=dim)).float()\n",
        "  return Q\n",
        "  \n",
        "def rot_trans_equivariance_unit_test(module, dataloader):\n",
        "    \"\"\"Unit test for checking whether a module (GNN layer) is \n",
        "    rotation and translation equivariant.\n",
        "    \"\"\"\n",
        "    it = iter(dataloader)\n",
        "    data = next(it)\n",
        "\n",
        "    out_1, pos_1 = module(data.x, data.pos, data.edge_index, data.edge_attr)\n",
        "\n",
        "    Q = random_orthogonal_matrix(dim=3)\n",
        "    t = torch.rand(3)\n",
        "\n",
        "    # Perform random rotation + translation on data.\n",
        "    data.pos = data.pos @ Q + t\n",
        "\n",
        "    out_2, pos_2 = module(data.x, data.pos, data.edge_index, data.edge_attr)\n",
        " \n",
        "    rotated_pos1 = pos_1@Q + t\n",
        "    return torch.allclose(rotated_pos1, pos_2, atol=1e-04)\n",
        " "
      ],
      "metadata": {
        "id": "FCgfDIu7rwkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddVirtualNodes:\n",
        "    \"\"\"\n",
        "    This transform adds up to 5 virtual nodes, depending on the number of input nodes\n",
        "    It uses METIS clustering to assign each node to a cluster, then fully connects\n",
        "    the virtual node to each cluster. Then, the virtual nodes are fully connected to\n",
        "    each other.\n",
        "\n",
        "    At least one virtual node is added to all molecules.  A virtual node is added for each 8 atoms in the dataset.\n",
        "    8 atoms was chosen b/c that, on average, adds 3 nodes to each molecule. \n",
        "    Visually plotting results on 20 molecules selected at random showed that the\n",
        "    clusters appeared to match chemical intuition.\n",
        "    \"\"\"\n",
        "    def __call__(self, data):\n",
        "        num_atoms = len(data.x)\n",
        "        assert num_atoms>0, \"Error: data should have more than 0 atoms\"\n",
        "\n",
        "        num_clusters = (num_atoms // 8) + 1\n",
        "\n",
        "        clusters = get_clusters(data, num_clusters)\n",
        "        new_data = data\n",
        "        for node_list in clusters.values():\n",
        "            new_data = add_virtual_node(new_data, node_list)\n",
        "        \n",
        "        return new_data"
      ],
      "metadata": {
        "id": "b3C1g-GNbJLY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddSingleVirtualNode:\n",
        "    \"\"\"\n",
        "    This transform adds a single virtual node that is fully connected to all nodes\n",
        "    \"\"\"\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        #Grab all nodes\n",
        "        node_list = np.arange(len(data.x))\n",
        "        \n",
        "        data = add_virtual_node(data, node_list)\n",
        "\n",
        "        return data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yky8Xd48I-4s"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [RUN] Download Dataset with virtual transform\n",
        "virtual_path = './qm9_virtual'\n",
        "default_path =  './qm9'\n",
        "target = 0\n",
        "\n",
        "# Transforms\n",
        "dense_transform = T.Compose([CompleteGraph(), SetTarget()])\n",
        "sparse_transform = SetTarget()\n",
        "virtual_transform = T.Compose([SetTarget(), AddVirtualNodes()]) #Removed CompleteGraph() to stop having them be fully connected\n",
        "single_virtual_transform = T.Compose([SetTarget(), AddSingleVirtualNode()])\n",
        "\n",
        "# Load versions of the QM9 dataset with the transforms defined\n",
        "virtual_dataset = QM9(virtual_path, pre_transform=virtual_transform)\n",
        "sparse_dataset = QM9(default_path, transform=sparse_transform)\n",
        "dense_dataset = QM9(default_path, transform=dense_transform)\n",
        "single_virtual_dataset = QM9(default_path, transform=single_virtual_transform)\n",
        "\n",
        "datasets = {'virtual': virtual_dataset, 'sparse': sparse_dataset, 'dense':dense_dataset, 'single': single_virtual_dataset}\n",
        "\n",
        "# Normalize targets per dataset to mean = 0 and std = 1.\n",
        "for key in datasets:\n",
        "    dataset = datasets[key]\n",
        "    mean = dataset.data.y.mean(dim=0, keepdim=True)\n",
        "    std = dataset.data.y.std(dim=0, keepdim=True)\n",
        "    dataset.data.y = (dataset.data.y - mean) / std\n",
        "\n",
        "#Grab std. Only doing this for one b/c \n",
        "#it's the same across datasets as all have same y\n",
        "\n",
        "mean, std = mean[:, target].item(), std[:, target].item()\n",
        "dataset[0]"
      ],
      "metadata": {
        "id": "EzoJBs9za2Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfa4bdf-b05c-4b06-9703-a594312a9d19"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[6, 12], edge_index=[2, 18], edge_attr=[18, 5], y=[1], pos=[6, 3], z=[6], name='gdb_1', idx=[1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "2TRkaCbOsHYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic MPNN"
      ],
      "metadata": {
        "id": "E3zQacNIqnN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        \"\"\"Message Passing Neural Network Layer\n",
        "\n",
        "        Arg?s:\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
        "        \"\"\"\n",
        "        # Set the aggregation function\n",
        "        super().__init__(aggr=aggr)\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        # MLP `\\psi` for computing messages `m_ij`\n",
        "        # Implemented as a stack of Linear->BN->ReLU->Linear->BN->ReLU\n",
        "        # dims: (2d + d_e) -> d\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
        "        # Implemented as a stack of Linear->BN->ReLU->Linear->BN->ReLU\n",
        "        # dims: 2d -> d\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "\n",
        "    def forward(self, h, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        The forward pass updates node features `h` via one round of message passing.\n",
        "\n",
        "        As our MPNNLayer class inherits from the PyG MessagePassing parent class,\n",
        "        we simply need to call the `propagate()` function which starts the \n",
        "        message passing procedure: `message()` -> `aggregate()` -> `update()`.\n",
        "        \n",
        "        The MessagePassing class handles most of the logic for the implementation.\n",
        "        To build custom GNNs, we only need to define our own `message()`, \n",
        "        `aggregate()`, and `update()` functions (defined subsequently).\n",
        "\n",
        "        Args:\n",
        "            h: (n, d) - initial node features\n",
        "            edge_index: (e, 2) - pairs of edges (i, j)\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "\n",
        "        Returns:\n",
        "            out: (n, d) - updated node features\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.propagate(edge_index, h=h, edge_attr=edge_attr)\n",
        "        return out\n",
        "\n",
        "    def message(self, h_i, h_j, edge_attr):\n",
        "        \"\"\"Step (1) Message\n",
        "\n",
        "        The `message()` function constructs messages from source nodes j \n",
        "        to destination nodes i for each edge (i, j) in `edge_index`.\n",
        "\n",
        "        The arguments can be a bit tricky to understand: `message()` can take \n",
        "        any arguments that were initially passed to `propagate`. Additionally, \n",
        "        we can differentiate destination nodes and source nodes by appending \n",
        "        `_i` or `_j` to the variable name, e.g. for the node features `h`, we\n",
        "        can use `h_i` and `h_j`. \n",
        "        \n",
        "        This part is critical to understand as the `message()` function\n",
        "        constructs messages for each edge in the graph. The indexing of the\n",
        "        original node features `h` (or other node variables) is handled under\n",
        "        the hood by PyG.\n",
        "\n",
        "        Args:\n",
        "            h_i: (e, d) - destination node features, essentially h[edge_index[0]]\n",
        "            h_j: (e, d) - source node features, essentially h[edge_index[1]]\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "        \n",
        "        Returns:\n",
        "            msg: (e, d) - messages `m_ij` passed through MLP `\\psi`\n",
        "        \"\"\"\n",
        "\n",
        "        msg = torch.cat([h_i, h_j, edge_attr], dim=-1)\n",
        "        return self.mlp_msg(msg)\n",
        "    \n",
        "    def aggregate(self, inputs, index):\n",
        "        \"\"\"Step (2) Aggregate\n",
        "\n",
        "        The `aggregate` function aggregates the messages from neighboring nodes,\n",
        "        according to the chosen aggregation function ('sum' by default).\n",
        "\n",
        "        Args:\n",
        "            inputs: (e, d) - messages `m_ij` from destination to source nodes\n",
        "            index: (e, 1) - list of source nodes for each edge/message in `input`\n",
        "\n",
        "        Returns:\n",
        "            aggr_out: (n, d) - aggregated messages `m_i`\n",
        "        \"\"\"\n",
        "        return scatter(inputs, index, dim=self.node_dim, reduce=self.aggr)\n",
        "    \n",
        "    def update(self, aggr_out, h):\n",
        "        \"\"\"\n",
        "        Step (3) Update\n",
        "\n",
        "        The `update()` function computes the final node features by combining the \n",
        "        aggregated messages with the initial node features.\n",
        "\n",
        "        `update()` takes the first argument `aggr_out`, the result of `aggregate()`, \n",
        "        as well as any optional arguments that were initially passed to \n",
        "        `propagate()`. E.g. in this case, we additionally pass `h`.\n",
        "\n",
        "        Args:\n",
        "            aggr_out: (n, d) - aggregated messages `m_i`\n",
        "            h: (n, d) - initial node features\n",
        "\n",
        "        Returns:\n",
        "            upd_out: (n, d) - updated node features passed through MLP `\\phi`\n",
        "        \"\"\"\n",
        "        upd_out = torch.cat([h, aggr_out], dim=-1)\n",
        "        return self.mlp_upd(upd_out)\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')"
      ],
      "metadata": {
        "id": "VcLeUV13mIbq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MPNNModel(Module):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1):\n",
        "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
        "\n",
        "        Args:\n",
        "            num_layers: (int) - number of message passing layers `L`\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            in_dim: (int) - initial node feature dimension `d_n`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            out_dim: (int) - output dimension (fixed to 1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Linear projection for initial node features\n",
        "        # dim: d_n -> d\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        \n",
        "        # Stack of MPNN layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(MPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "        \n",
        "        # Global pooling/readout function `R` (mean pooling)\n",
        "        # PyG handles the underlying logic via `global_mean_pool()`\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Linear prediction head\n",
        "        # dim: d -> out_dim\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: (PyG.Data) - batch of PyG graphs\n",
        "\n",
        "        Returns: \n",
        "            out: (batch_size, out_dim) - prediction for each graph\n",
        "        \"\"\"\n",
        "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            h = h + conv(h, data.edge_index, data.edge_attr) # (n, d) -> (n, d)\n",
        "            # Note that we add a residual connection after each MPNN layer\n",
        "\n",
        "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
        "\n",
        "        out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
        "\n",
        "        return out.view(-1)"
      ],
      "metadata": {
        "id": "JevjpzQ9lwLR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EGNN from Satorras et al."
      ],
      "metadata": {
        "id": "NL82l51GslOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EquivariantMPNNLayer(MessagePassing):\n",
        "    def __init__(self, emb_dim=64, edge_dim=4, aggr='add'):\n",
        "        \"\"\"Message Passing Neural Network Layer\n",
        "\n",
        "        This layer is equivariant to 3D rotations and translations.\n",
        "\n",
        "        Args:\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
        "        \"\"\"\n",
        "        # Set the aggregation function\n",
        "        super().__init__(aggr=aggr)\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        # Define the MLPs constituting your new layer.\n",
        "        # At the least, you will need `\\psi` and `\\phi` \n",
        "        # (but their definitions may be different from what\n",
        "        # we used previously).\n",
        "        #\n",
        "\n",
        "        # MLP `\\psi_m` for computing feature messages `m_ij`\n",
        "        # dims: 2d + d_e + 1 -> d, \n",
        "        # +1 comes from distance btwn nodes\n",
        "\n",
        "\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim + 1, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "\n",
        "        # MLP `\\psi_x` for computing the weight of relative difference of coord\n",
        "        # dims: d -> 1, \n",
        "        # +1 comes from distance btwn nodes\n",
        "        self.mlp_coord = Linear(emb_dim, 1) #As simple as possible for now\n",
        "        \n",
        "\n",
        "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
        "        # dims: 2d -> d\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        # ===========================================\n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        The forward pass updates node features `h` via one round of message passing.\n",
        "\n",
        "        Args:\n",
        "            h: (n, d) - initial node features\n",
        "            pos: (n, 3) - initial node coordinates\n",
        "            edge_index: (e, 2) - pairs of edges (i, j)\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "\n",
        "        Returns:\n",
        "            out: [(n, d),(n,3)] - updated node features and coordinates\n",
        "        \"\"\"\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        # Notice that the `forward()` function has a new argument \n",
        "        # `pos` denoting the initial node coordinates. Your task is\n",
        "        # to update the `propagate()` function in order to pass `pos`\n",
        "        # to the `message()` function along with the other arguments.\n",
        "        #\n",
        "\n",
        "        #Same as invariantCoordMPNN\n",
        "        feat_upd, coord_upd = self.propagate(edge_index, h=h, edge_attr = edge_attr, pos = pos)\n",
        "        new_coords = coord_upd + pos\n",
        "        return [feat_upd, new_coords]\n",
        "        # ==========================================\n",
        "\n",
        "    # ============ YOUR CODE HERE ==============\n",
        "    # Write custom `message()`, `aggregate()`, and `update()` functions\n",
        "    # which ensure that the layer is 3D rotation and translation equivariant.\n",
        "    \n",
        "    def message(self, h_i, h_j, pos_i, pos_j, edge_attr):\n",
        "        \"\"\"The `message()` function constructs messages from source nodes j \n",
        "        to destination nodes i for each edge (i, j) in `edge_index`.\n",
        "        \n",
        "        Args:\n",
        "            h_i: (e, d) - destination node features, essentially h[edge_index[0]]\n",
        "            h_j: (e, d) - source node features, essentially h[edge_index[1]]\n",
        "            pos_i: (e, 3) - destination node position, essentially pos[edge_index[0]]\n",
        "            pos_j: (e, 3) - source node position, essentially pos[edge_index[1]]\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "            \n",
        "        Returns:\n",
        "            msg: (e, d) - messages `m_ij` passed through MLP `\\psi`\n",
        "            coord_update: (e, 3)- scalar weighting coefficient times dif in vectors\n",
        "                    (x_i-x_j)*psi_x(m_ij)\n",
        "        \"\"\"\n",
        "\n",
        "        dist = torch.sqrt(torch.sum(torch.pow((pos_i-pos_j),2), dim = 1)).unsqueeze(dim = 1) #Compute L2-norm\n",
        "        msg = torch.cat([h_i, h_j, edge_attr, dist], dim = -1) #has distance now\n",
        "        msg = self.mlp_msg(msg)\n",
        "\n",
        "        coord_weight = self.mlp_coord(msg)\n",
        "        # print(\"coord weight shape:\", coord_weight.shape)\n",
        "        coord_update = (pos_i-pos_j) * coord_weight\n",
        "        # print(\"coord update shape:\", coord_update.shape)\n",
        "        # assert coord_update.shape == [len(msg), 3]\n",
        "        return [msg, coord_update]\n",
        "    \n",
        "    def aggregate(self, inputs, index):\n",
        "        \"\"\"The `aggregate` function aggregates the messages from neighboring nodes,\n",
        "        according to the chosen aggregation function ('sum' by default).\n",
        "\n",
        "        Args:\n",
        "            inputs: [(e, d), (e,3)] - \n",
        "                tuple of:\n",
        "                    [0] messages `m_ij` from destination to source nodes,\n",
        "                    [1] coord messages from destination to source nodes\n",
        "            index: (e, 1) - list of source nodes for each edge/message in `input`\n",
        "\n",
        "        Returns:\n",
        "            feat_out: (n, d) - aggregated messages `m_i`\n",
        "            coord_out: (n, 3) - aggregated coordinate update \n",
        "        \"\"\"\n",
        "\n",
        "        feat_out = scatter(inputs[0], index, dim=self.node_dim, reduce=self.aggr)\n",
        "        coord_out = scatter(inputs[1], index, dim=self.node_dim, reduce= 'mean') \n",
        "        #I believe mean is same here as the original paper, which sums then \n",
        "        #divides by number of elements\n",
        "\n",
        "\n",
        "        return [feat_out, coord_out]\n",
        "\n",
        "    def update(self, inputs, h):\n",
        "        \"\"\"The `update()` function computes the final node features by combining the \n",
        "        aggregated messages with the initial node features.\n",
        "\n",
        "        Args:\n",
        "            inputs: [(e, d), (e,3)] - \n",
        "                tuple of:\n",
        "                    [0] aggregated messages `m_i`\n",
        "                    [1] aggregated coordinate updates\n",
        "            h: (n, d) - initial node features\n",
        "\n",
        "        Returns:\n",
        "            upd_feat: (n, d) - updated node features passed through MLP `\\phi`\n",
        "            upd_coord: (n, d) - updated node coordinates from aggregator\n",
        "        \"\"\"\n",
        "        upd_feat = torch.cat([h, inputs[0]], dim=-1) #concats original feature and message\n",
        "        upd_feat = self.mlp_upd(upd_feat)\n",
        "\n",
        "        upd_coord = inputs[1]\n",
        "        assert upd_coord.shape[1] == 3 #coordinates are X,Y,Z so size 3\n",
        "        return [upd_feat, upd_coord]\n",
        "    # ==========================================\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
        "\n",
        "\n",
        "class EGNN(MPNNModel):\n",
        "    def __init__(self, num_layers=4, emb_dim=64, in_dim=12, edge_dim=4, out_dim=1):\n",
        "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
        "\n",
        "        This model uses both node features and coordinates as inputs, and\n",
        "        is invariant to 3D rotations and translations (the constituent MPNN layers\n",
        "        are equivariant to 3D rotations and translations).\n",
        "\n",
        "        Args:\n",
        "            num_layers: (int) - number of message passing layers `L`\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            in_dim: (int) - initial node feature dimension `d_n`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            out_dim: (int) - output dimension (fixed to 1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Linear projection for initial node features\n",
        "        # dim: d_n -> d\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        \n",
        "        # Stack of MPNN layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(EquivariantMPNNLayer(emb_dim, edge_dim, aggr='add'))\n",
        "        \n",
        "        # Global pooling/readout function `R` (mean pooling)\n",
        "        # PyG handles the underlying logic via `global_mean_pool()`\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Linear prediction head\n",
        "        # dim: d -> out_dim\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: (PyG.Data) - batch of PyG graphs\n",
        "\n",
        "        Returns: \n",
        "            out: (batch_size, out_dim) - prediction for each graph\n",
        "        \"\"\"\n",
        "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
        "        pos = data.pos\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            # Message passing layer\n",
        "            h_update, pos_update = conv(h, pos, data.edge_index, data.edge_attr)\n",
        "            \n",
        "            # Update node features\n",
        "            h = h + h_update # (n, d) -> (n, d)\n",
        "            # Note that we add a residual connection after each MPNN layer\n",
        "            \n",
        "            # Update node coordinates\n",
        "            pos = pos_update # (n, 3) -> (n, 3)\n",
        "\n",
        "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
        "\n",
        "        out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
        "\n",
        "        return out.view(-1)"
      ],
      "metadata": {
        "id": "aU5IjayDlLab"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESULTS = {}\n",
        "DF_RESULTS = pd.DataFrame(columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])"
      ],
      "metadata": {
        "id": "q45xuBOXmmq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "model = EGNN(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "# ==========================================\n",
        "\n",
        "model_name = type(model).__name__\n",
        "# best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "#     model, \n",
        "#     model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "#     train_loader,\n",
        "#     val_loader, \n",
        "#     test_loader,\n",
        "#     n_epochs=100\n",
        "# )\n",
        "\n",
        "# RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "# df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "# DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ],
      "metadata": {
        "id": "SLX-sJGRmN3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom EGNN that optionally looks at coordinate information of virtual nodes"
      ],
      "metadata": {
        "id": "YQBzs7QGq38j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "h_i^{\\ell+1} = \\phi \\Bigg( h_i^{\\ell}, \\oplus_{j \\in {N}_i} \\Big( \\psi_{real} \\left( h_i^{\\ell}, h_j^{\\ell}, e_{ij},  \\lvert\\lvert {{x_i} - {x_j}}\\rvert\\rvert^2 \\right),\\psi_{virtual} \\left( h_i^{\\ell}, h_j^{\\ell}, e_{ij},  \\color{purple}{\\lvert\\lvert {{x_i} - {x_j}}\\rvert\\rvert^2} \\right) \\Big) \\Bigg),\n",
        "$$"
      ],
      "metadata": {
        "id": "4JGXgzbGVsNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VirtualEquivariantMPNNLayer(MessagePassing):\n",
        "    def __init__(self, virt_pos:bool, emb_dim=64, edge_dim=5, aggr='add'):\n",
        "        \"\"\"Message Passing Neural Network Layer\n",
        "\n",
        "        This layer is equivariant to 3D rotations and translations.\n",
        "\n",
        "        This layer optionally will use coordinate information \n",
        "\n",
        "        Args:\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            aggr: (str) - aggregation function `\\oplus` (sum/mean/max)\n",
        "        \"\"\"\n",
        "        # Set the aggregation function\n",
        "        super().__init__(aggr=aggr)\n",
        "\n",
        "        self.emb_dim = emb_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        self.virt_pos = virt_pos\n",
        "\n",
        "\n",
        "        # ============ YOUR CODE HERE ==============\n",
        "        # Define the MLPs constituting your new layer.\n",
        "        # At the least, you will need `\\psi` and `\\phi` \n",
        "        # (but their definitions may be different from what\n",
        "        # we used previously).\n",
        "        #\n",
        "\n",
        "        # MLP `\\psi_m` for computing feature messages `m_ij`\n",
        "        # dims: 2d + d_e + 1 -> d, \n",
        "        # +1 comes from distance btwn nodes\n",
        "\n",
        "\n",
        "        self.mlp_msg = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim + 1, emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "\n",
        "        #Size depends on virt_pos, whether model should use virtual coordinates.\n",
        "        #Otherwise same structure as for the generic MLP message\n",
        "        self.mlp_virt = Sequential(\n",
        "            Linear(2*emb_dim + edge_dim + int(virt_pos), emb_dim), BatchNorm1d(emb_dim), ReLU(),\n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "\n",
        "        # MLP `\\psi_x` for computing the weight of relative difference of coord\n",
        "        # Will be used with virtual and 'real' messages\n",
        "        # dims: d -> 1, \n",
        "        \n",
        "        self.mlp_coord = Linear(emb_dim, 1) #As simple as possible for now\n",
        "        \n",
        "\n",
        "        # MLP `\\phi` for computing updated node features `h_i^{l+1}`\n",
        "        # dims: 2d -> d\n",
        "        self.mlp_upd = Sequential(\n",
        "            Linear(2*emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU(), \n",
        "            Linear(emb_dim, emb_dim), BatchNorm1d(emb_dim), ReLU()\n",
        "          )\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        # ===========================================\n",
        "\n",
        "    def forward(self, h, pos, edge_index, edge_attr):\n",
        "        \"\"\"\n",
        "        The forward pass updates node features `h` via one round of message passing.\n",
        "\n",
        "        Args:\n",
        "            h: (n, d) - initial node features\n",
        "            pos: (n, 3) - initial node coordinates\n",
        "            edge_index: (e, 2) - pairs of edges (i, j)\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "\n",
        "        Returns:\n",
        "            out: [(n, d),(n,3)] - updated node features and coordinates\n",
        "        \"\"\"\n",
        "\n",
        "        feat_upd, coord_upd = self.propagate(edge_index, h=h, edge_attr = edge_attr, pos = pos)\n",
        "        new_coords = coord_upd + pos\n",
        "        return [feat_upd, new_coords]\n",
        "\n",
        "    \n",
        "    def message(self, h_i, h_j, pos_i, pos_j, edge_attr, edge_index):\n",
        "        \"\"\"The `message()` function constructs messages from source nodes j \n",
        "        to destination nodes i for each edge (i, j) in `edge_index`.\n",
        "        \n",
        "        Args:\n",
        "            h_i: (e, d) - destination node features, essentially h[edge_index[0]]\n",
        "            h_j: (e, d) - source node features, essentially h[edge_index[1]]\n",
        "            pos_i: (e, 3) - destination node position, essentially pos[edge_index[0]]\n",
        "            pos_j: (e, 3) - source node position, essentially pos[edge_index[1]]\n",
        "            edge_attr: (e, d_e) - edge features\n",
        "            \n",
        "        Returns:\n",
        "            msg: (e, d) - messages `m_ij` passed through MLP `\\psi`\n",
        "            coord_update: (e, 3)- scalar weighting coefficient times dif in vectors\n",
        "                    (x_i-x_j)*psi_x(m_ij)\n",
        "            receiver_index: (e,1) - list of receiving nodes for each edge. Used\n",
        "                    because the appended messages are now out of order, so the \n",
        "                    default from MessagePassing no longer applies\n",
        "            coord_index: (e_real + e_virtual (if using position),1)\n",
        "                    list of nodes to receiving nodes for the coordinate updates\n",
        "                    note that this can be smaller than e depending on if the\n",
        "                    virtual nodes' positions are being used or not\n",
        "        \"\"\"\n",
        "        ########### Separate into 'virtual' and 'real' edges ###########\n",
        "\n",
        "        virtual_edge_pos = -1 #ASSUMES THE VIRTUAL EDGE IS LAST INDEX. \n",
        "\n",
        "        #Get indices\n",
        "        real_ind = torch.nonzero(edge_attr[:, virtual_edge_pos] == 0, as_tuple=False).squeeze()\n",
        "        virt_ind = torch.nonzero(edge_attr[:, virtual_edge_pos] == 1, as_tuple=False).squeeze()\n",
        "        \n",
        "        #Separate everything based on which MLP it will go to\n",
        "        h_i_real = torch.index_select(h_i, dim=0, index=real_ind)\n",
        "        h_j_real = torch.index_select(h_j, dim=0, index=real_ind)\n",
        "        pos_i_real = torch.index_select(pos_i, dim=0, index=real_ind)\n",
        "        pos_j_real = torch.index_select(pos_j, dim=0, index=real_ind)\n",
        "        edge_attr_real = torch.index_select(edge_attr, dim=0, index=real_ind)\n",
        "\n",
        "        h_i_virt = torch.index_select(h_i, dim=0, index=virt_ind)\n",
        "        h_j_virt = torch.index_select(h_j, dim=0, index=virt_ind)\n",
        "        pos_i_virt = torch.index_select(pos_i, dim=0, index=virt_ind)\n",
        "        pos_j_virt = torch.index_select(pos_j, dim=0, index=virt_ind)\n",
        "        edge_attr_virt = torch.index_select(edge_attr, dim=0, index=virt_ind)\n",
        "\n",
        "        ############ Compute message over real edges ##################\n",
        "        dist_real = torch.sqrt(torch.sum(torch.pow((pos_i_real-pos_j_real),2), dim = 1)).unsqueeze(dim = 1) #Compute L2-norm\n",
        "        \n",
        "        input_real = torch.cat([h_i_real, h_j_real, edge_attr_real, dist_real], dim = -1) #has distance now\n",
        "        msg_real = self.mlp_msg(input_real)\n",
        "\n",
        "        coord_weight_real = self.mlp_coord(msg_real)\n",
        "        coord_update_real = (pos_i_real-pos_j_real) * coord_weight_real\n",
        "\n",
        "        ########### Compute message over virtual edges ###############\n",
        "\n",
        "        input_virt = torch.cat([h_i_virt, h_j_virt, edge_attr_virt], dim = -1) \n",
        "        if self.virt_pos:\n",
        "            dist_virt = torch.sqrt(torch.sum(torch.pow((pos_i_virt-pos_j_virt),2), dim = 1)).unsqueeze(dim = 1)\n",
        "            input_virt = torch.cat([input_virt, dist_virt], dim = -1)\n",
        "        \n",
        "        msg_virt = self.mlp_virt(input_virt)\n",
        "\n",
        "        if self.virt_pos:\n",
        "            coord_weight_virt = self.mlp_coord(msg_virt)\n",
        "            coord_update_virt = (pos_i_virt-pos_j_virt) * coord_weight_virt\n",
        "\n",
        "        ########## Combine messages over both real and virtual edges and make indices ######\n",
        "        msg = torch.cat((msg_real, msg_virt), dim = 0)\n",
        "        assert msg.shape == (len(h_i), self.emb_dim), \"message has the wrong shape. Expected {}x{} but found {}\".format(len(h_i), self.emb_dim, msg.shape)\n",
        "\n",
        "        indices = torch.cat((real_ind, virt_ind))\n",
        "        receiver_index = edge_index[0, indices]\n",
        "\n",
        "        coord_update = coord_update_real\n",
        "        coord_index = edge_index[0, real_ind]\n",
        "\n",
        "        if self.virt_pos:\n",
        "            coord_update = torch.cat((coord_update, coord_update_virt))\n",
        "            coord_index = receiver_index\n",
        "\n",
        "\n",
        "\n",
        "        return [msg, coord_update, receiver_index, coord_index]\n",
        "    \n",
        "    def aggregate(self, inputs, index):\n",
        "        \"\"\"The `aggregate` function aggregates the messages from neighboring nodes,\n",
        "        according to the chosen aggregation function ('sum' by default).\n",
        "\n",
        "        Args:\n",
        "            inputs: [(e, d), (e,3), (e,1), (<=e,1)] - \n",
        "                tuple of:\n",
        "                    [0] messages `m_ij` from destination to source nodes,\n",
        "                    [1] coord messages from destination to source nodes\n",
        "                    [2] index for messages\n",
        "                    [3] index for coordinate messages\n",
        "            index: (e, 1) - list of source nodes for each edge/message in `input`\n",
        "\n",
        "        Returns:\n",
        "            feat_out: (n, d) - aggregated messages `m_i`\n",
        "            coord_out: (n, 3) - aggregated coordinate update \n",
        "        \"\"\"\n",
        "        \n",
        "        #I need to make sure all coordinate updates are present before going into\n",
        "        #scatter so that the coordinate update is computed for all nodes\n",
        "        coord_index = inputs[3]\n",
        "        if not self.virt_pos:   \n",
        "            #Coord update is only over the real parts, which conveniently\n",
        "            #matches the first part of receiver_index\n",
        "            #First make an zeros of the size of msg over all nodes\n",
        "            full_sized_coord_update = torch.zeros((inputs[0].shape[0], 3), device = torch.device('cuda'))\n",
        "            #Update the first part to be equal to the coordinate messages\n",
        "            #leave the rest as zeros, since I don't want the coordinates of the virtual\n",
        "            #nodes to be updated\n",
        "            full_sized_coord_update[:len(inputs[1])] = inputs[1]\n",
        "            #Use the fact that the receiver_index is \n",
        "            #cat(real_rec_edge_ind, virt_rec_edge_ind)\n",
        "            #and set coord_index to the receiver index for feature messages\n",
        "            coord_index = inputs[2]            \n",
        "\n",
        "        #my index is not in ascending order, but scatter convienently reorders\n",
        "        #so that the output matches the node order of the original data\n",
        "        feat_out = scatter(inputs[0], inputs[2], dim=self.node_dim, reduce=self.aggr)\n",
        "        if self.virt_pos:\n",
        "            coord_out = scatter(inputs[1], coord_index, dim=self.node_dim, reduce= 'mean')\n",
        "        else:\n",
        "            coord_out = scatter(full_sized_coord_update, coord_index, dim=self.node_dim, reduce= 'mean')\n",
        "\n",
        "        return [feat_out, coord_out]\n",
        "\n",
        "    def update(self, inputs, h):\n",
        "        \"\"\"The `update()` function computes the final node features by combining the \n",
        "        aggregated messages with the initial node features.\n",
        "\n",
        "        Args:\n",
        "            inputs: [(e, d), (e,3)] - \n",
        "                tuple of:\n",
        "                    [0] aggregated messages `m_i`\n",
        "                    [1] aggregated coordinate updates\n",
        "            h: (n, d) - initial node features\n",
        "\n",
        "        Returns:\n",
        "            upd_feat: (n, d) - updated node features passed through MLP `\\phi`\n",
        "            upd_coord: (n, d) - updated node coordinates from aggregator\n",
        "        \"\"\"\n",
        "        upd_feat = torch.cat([h, inputs[0]], dim=-1)\n",
        "        upd_feat = self.mlp_upd(upd_feat)\n",
        "\n",
        "        upd_coord = inputs[1]\n",
        "        assert upd_coord.shape[1] == 3\n",
        "        return [upd_feat, upd_coord]\n",
        "    # ==========================================\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f'{self.__class__.__name__}(emb_dim={self.emb_dim}, aggr={self.aggr})')\n",
        "\n",
        "\n",
        "class VirtualEGNN(MPNNModel):\n",
        "    def __init__(self, virt_pos:bool, num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1):\n",
        "        \"\"\"Message Passing Neural Network model for graph property prediction\n",
        "\n",
        "        This model uses both node features and coordinates as inputs, and\n",
        "        is invariant to 3D rotations and translations (the constituent MPNN layers\n",
        "        are equivariant to 3D rotations and translations).\n",
        "\n",
        "        Args:\n",
        "            num_layers: (int) - number of message passing layers `L`\n",
        "            emb_dim: (int) - hidden dimension `d`\n",
        "            in_dim: (int) - initial node feature dimension `d_n`\n",
        "            edge_dim: (int) - edge feature dimension `d_e`\n",
        "            out_dim: (int) - output dimension (fixed to 1)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Linear projection for initial node features\n",
        "        # dim: d_n -> d\n",
        "        self.lin_in = Linear(in_dim, emb_dim)\n",
        "        \n",
        "        # Stack of MPNN layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        for layer in range(num_layers):\n",
        "            self.convs.append(VirtualEquivariantMPNNLayer(virt_pos= virt_pos, emb_dim = emb_dim, edge_dim = edge_dim, aggr='add'))\n",
        "        \n",
        "        # Global pooling/readout function `R` (mean pooling)\n",
        "        # PyG handles the underlying logic via `global_mean_pool()`\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Linear prediction head\n",
        "        # dim: d -> out_dim\n",
        "        self.lin_pred = Linear(emb_dim, out_dim)\n",
        "        \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: (PyG.Data) - batch of PyG graphs\n",
        "\n",
        "        Returns: \n",
        "            out: (batch_size, out_dim) - prediction for each graph\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.lin_in(data.x) # (n, d_n) -> (n, d)\n",
        "        pos = data.pos\n",
        "        \n",
        "        for conv in self.convs:\n",
        "            # Message passing layer\n",
        "            h_update, pos_update = conv(h, pos, data.edge_index, data.edge_attr)\n",
        "            \n",
        "            # Update node features\n",
        "            h = h + h_update # (n, d) -> (n, d)\n",
        "            # Note that we add a residual connection after each MPNN layer \n",
        "            pos = pos_update # (n, 3) -> (n, 3)\n",
        "\n",
        "        h_graph = self.pool(h, data.batch) # (n, d) -> (batch_size, d)\n",
        "\n",
        "        out = self.lin_pred(h_graph) # (batch_size, d) -> (batch_size, 1)\n",
        "\n",
        "        return out.view(-1)"
      ],
      "metadata": {
        "id": "UQG1ccUGrDnu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = VirtualEGNN(virt_pos = False, num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "# model_name = type(model).__name__\n",
        "# best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "#     model, \n",
        "#     'Without Virtual Positions', # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "#     train_loader,\n",
        "#     val_loader, \n",
        "#     test_loader,\n",
        "#     n_epochs=100\n",
        "# )\n",
        "# RESULTS[model_name] = (best_val_error, test_error, train_time)\n",
        "# df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\"])\n",
        "# DF_RESULTS = DF_RESULTS.append(df_temp, ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "1OEXg-zXxjzb",
        "outputId": "6bfb8037-68f2-4256-a008-708ddb74d156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment for Without Virtual Positions, training on 1000 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEquivariantMPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch Time: 0.03 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.03 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.03 mins\n",
            "Epoch Time: 0.03 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.3461931, Val MAE: 0.8547680, Test MAE: 0.6579633\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.02 mins\n",
            "Epoch Time: 0.02 mins\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-58a5d3647bc7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVirtualEquivariantMPNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvirt_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'Without Virtual Positions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# \"MPNN w/ Features and Coordinates (Equivariant Layers)\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4150b9f3cb9d>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model, model_name, train_loader, val_loader, test_loader, n_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Train model for one epoch, return avg. training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Evaluate model on validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4150b9f3cb9d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss_all\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluating Models\n",
        "\n",
        "I now have a 4 datasets with different characteristics:\n",
        "1. Fully-connected Graph: dense_dataset\n",
        "2. Sparse, original graph: sparse_dataset\n",
        "3. Single virtual node: single_virtual_dataset\n",
        "4. Multiple Virtual nodes: virtual_dataset\n",
        "\n",
        "I also have multiple models:\n",
        "* (a) A Basic MPNN that ignores positional info\n",
        "* (b) An Equivariant MPNN, EGNN\n",
        "* (c) A Custom EGNN which uses a different message for the virtual nodes, and optionally considers (and updates) the position of the virtual nodes themselves.\n",
        "\n",
        "I now want to test the following combinations\n",
        "\n",
        "* (a)\n",
        "    * 1, 2, 3, 4\n",
        "* (b)\n",
        "    * 1, 2, 3, 4\n",
        "* (c w/ position)\n",
        "    * 3, 4 \n",
        "* (c w/out position)\n",
        "    * 3, 4\n",
        "\n",
        "I want to test each combination 3 times so my graphs will have error bars\n",
        "\n",
        "I also want to record the following attributes for each training run:\n",
        "* Model name\n",
        "* Dataset\n",
        "* Run number\n",
        "* Val MAE per epoch\n",
        "* Test MAE per epoch \n",
        "\n",
        "* Best validation MAE\n",
        "* Test MAE for best validation epoch\n",
        "* Total Train Time (for tracking computational cost)\n",
        "\n",
        "run_experiment returns best val error, test error, train time\n",
        "\n",
        "perf_per_epoch can return the test error, val error, epoch, model_name\n",
        "\n",
        "I will manually count the runs and then that should do it\n"
      ],
      "metadata": {
        "id": "H4LhFwXKtY_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_test_split(dataset):\n",
        "    \"\"\"\n",
        "    Returns the dataloaders for train, validation, and testing\n",
        "    \"\"\"\n",
        "    NUM_WORKERS = 0 #surprisingly found this was the fastest in this case\n",
        "\n",
        "    # Split datasets (our 3K subset)\n",
        "    train_dataset = dataset[:5000]\n",
        "    val_dataset = dataset[5000:7500]\n",
        "    test_dataset = dataset[7500:10000]\n",
        "    print(f\"Created dataset splits with {len(train_dataset)} training, {len(val_dataset)} validation, {len(test_dataset)} test samples.\")\n",
        "\n",
        "    # Create dataloaders with batch size = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers =NUM_WORKERS, pin_memory = True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers = NUM_WORKERS, pin_memory = True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers = NUM_WORKERS, pin_memory = True)\n",
        "\n",
        "    return (train_loader, val_loader, test_loader)"
      ],
      "metadata": {
        "id": "CR-DImFq3knL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(dataset, model, model_name, dataset_name, run_number = 1):\n",
        "\n",
        "    #Split data and get dataloaders\n",
        "    train_loader, val_loader, test_loader = train_val_test_split(dataset)\n",
        "\n",
        "    model_name = type(model).__name__\n",
        "    best_val_error, test_error, train_time, perf_per_epoch = run_experiment(\n",
        "        model, \n",
        "        model_name, # \"MPNN w/ Features and Coordinates (Equivariant Layers)\", \n",
        "        dataset_name,\n",
        "        train_loader,\n",
        "        val_loader, \n",
        "        test_loader,\n",
        "        n_epochs=100,\n",
        "    )\n",
        "    \n",
        "    #Save Epoch results in a file\n",
        "    file_name = model_name + '_' + dataset_name + '_' + str(run_number) + '.csv'\n",
        "    folder_path = '/content/drive/MyDrive/L45 Representation Learning on Graphs/Mini Project/Data/QM9 Results'\n",
        "    full_path = os.path.join(folder_path, file_name)\n",
        "    \n",
        "    df_temp = pd.DataFrame(perf_per_epoch, columns=[\"Test MAE\", \"Val MAE\", \"Epoch\", \"Model\", \"Dataset\"])\n",
        "    df_temp.to_csv(full_path, index = False)\n",
        "\n",
        "    return best_val_error, test_error, train_time"
      ],
      "metadata": {
        "id": "U74JRdB33HSL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test all models\n",
        "RESULTS = {}\n",
        "datasets = {'virtual': virtual_dataset, 'sparse': sparse_dataset, 'dense':dense_dataset, 'single': single_virtual_dataset}\n",
        "\n",
        "experiment_combos = {'VirtualEGNN w pos': ['virtual', 'single'],\n",
        "                     'VirtualEGNN w/out pos' : ['virtual', 'single'],\n",
        "                     'EGNN': ['virtual', 'single', 'dense', 'sparse'],\n",
        "                     'BaseMPNN': ['virtual', 'single', 'dense', 'sparse']}\n",
        "\n",
        "#loop through models\n",
        "for model_name in experiment_combos:\n",
        "    #loop through datasets\n",
        "    for dataset_name in experiment_combos[model_name]:\n",
        "        #loop through replicates\n",
        "        for replicate in np.arange(1,4):\n",
        "            \n",
        "            #initialize model depending on model_name, and dataset since virtual_nodes have different size\n",
        "            if model_name == 'VirtualEGNN w pos':\n",
        "                model = VirtualEGNN(virt_pos = True, num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "\n",
        "            elif model_name == 'VirtualEGNN w/out pos':\n",
        "                model = VirtualEGNN(virt_pos = False, num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "\n",
        "            elif model_name == 'EGNN' and dataset_name == 'virtual':\n",
        "                model = EGNN(num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "            elif model_name == 'EGNN' and dataset_name == 'single':\n",
        "                model = EGNN(num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "            elif model_name == 'EGNN':\n",
        "                model = EGNN(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "            elif model_name == 'BaseMPNN' and dataset_name == 'virtual':\n",
        "                model = MPNNModel(num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "            elif model_name == 'BaseMPNN' and dataset_name == 'single':\n",
        "                model = MPNNModel(num_layers=4, emb_dim=64, in_dim=12, edge_dim=5, out_dim=1)\n",
        "            elif model_name == 'BaseMPNN':\n",
        "                model = MPNNModel(num_layers=4, emb_dim=64, in_dim=11, edge_dim=4, out_dim=1)\n",
        "\n",
        "            \n",
        "            temp_dataset = datasets[dataset_name]\n",
        "\n",
        "            best_val_error, test_error, train_time = test_model(temp_dataset, model, model_name, dataset_name, replicate)\n",
        "            exp_name = model_name + '_' + dataset_name + '_' + str(replicate)\n",
        "            RESULTS[exp_name] = (best_val_error, test_error, train_time)\n",
        "\n",
        "            folder_path = '/content/drive/MyDrive/L45 Representation Learning on Graphs/Mini Project/Data/QM9 Results'\n",
        "            results_filename = 'summary_results.csv'\n",
        "            results_df = pd.DataFrame(RESULTS, index = ['Best Val MAE', 'Test MAE', 'Training Time'])\n",
        "            results_df.transpose(copy = True)\n",
        "            results_df.to_csv(os.path.join(folder_path, results_filename), index = True, header = True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQFiMFMu8swe",
        "outputId": "f96398f1-e468-4a5f-c5b2-1d465e1c9dc9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156293\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0022691, Val MAE: 1.1306154, Test MAE: 1.3888992\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0002463, Val MAE: 1.0468374, Test MAE: 1.2244560\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0000567, Val MAE: 1.0162082, Test MAE: 1.1735643\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000345, Val MAE: 1.0915610, Test MAE: 1.1892811\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0000073, Val MAE: 1.0817896, Test MAE: 1.1892811\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000023, Val MAE: 1.0730246, Test MAE: 1.1892811\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000009, Val MAE: 1.0675504, Test MAE: 1.1892811\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000002, Val MAE: 1.0642663, Test MAE: 1.1892811\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000001, Val MAE: 1.0635541, Test MAE: 1.1892811\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000000, Val MAE: 1.0630804, Test MAE: 1.1892811\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.9983852, corresponding test MAE: 1.1892811.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156293\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0221088, Val MAE: 1.1612423, Test MAE: 1.5425203\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0007836, Val MAE: 1.0151930, Test MAE: 1.2656913\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001474, Val MAE: 0.7930028, Test MAE: 1.3268053\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000552, Val MAE: 0.4745355, Test MAE: 1.4006377\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0000145, Val MAE: 0.4534809, Test MAE: 1.4270298\n",
            "Epoch: 060, LR: 0.000900, Loss: 0.0000072, Val MAE: 0.4803051, Test MAE: 1.4270298\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000022, Val MAE: 0.5002644, Test MAE: 1.4270298\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0000006, Val MAE: 0.5099322, Test MAE: 1.4270298\n",
            "Epoch: 090, LR: 0.000531, Loss: 0.0000001, Val MAE: 0.5157130, Test MAE: 1.4270298\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0000001, Val MAE: 0.5179044, Test MAE: 1.4270298\n",
            "\n",
            "Done! Training took 0.08 mins. Best validation MAE: 0.4476680, corresponding test MAE: 1.4270298.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156293\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0901243, Val MAE: 1.1485369, Test MAE: 1.7550884\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0237138, Val MAE: 1.0512155, Test MAE: 1.3688884\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0013771, Val MAE: 0.9744116, Test MAE: 1.3562979\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0004470, Val MAE: 0.5719791, Test MAE: 1.8778400\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0001113, Val MAE: 0.5138428, Test MAE: 2.0434645\n",
            "Epoch: 060, LR: 0.000900, Loss: 0.0000429, Val MAE: 0.5332273, Test MAE: 2.0584541\n",
            "Epoch: 070, LR: 0.000810, Loss: 0.0000130, Val MAE: 0.5442884, Test MAE: 2.0584541\n",
            "Epoch: 080, LR: 0.000656, Loss: 0.0000014, Val MAE: 0.5529753, Test MAE: 2.0584541\n",
            "Epoch: 090, LR: 0.000531, Loss: 0.0000003, Val MAE: 0.5553320, Test MAE: 2.0584541\n",
            "Epoch: 100, LR: 0.000478, Loss: 0.0000004, Val MAE: 0.5562334, Test MAE: 2.0584541\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.5104019, corresponding test MAE: 2.0584541.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156293\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0225557, Val MAE: 1.0519728, Test MAE: 1.7154160\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0009587, Val MAE: 0.9225230, Test MAE: 1.3813943\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0000457, Val MAE: 1.1893316, Test MAE: 1.3813943\n",
            "Epoch: 040, LR: 0.000729, Loss: 0.0000642, Val MAE: 1.4232123, Test MAE: 1.3813943\n",
            "Epoch: 050, LR: 0.000656, Loss: 0.0000075, Val MAE: 1.4665369, Test MAE: 1.3813943\n",
            "Epoch: 060, LR: 0.000531, Loss: 0.0000039, Val MAE: 1.4028803, Test MAE: 1.3813943\n",
            "Epoch: 070, LR: 0.000430, Loss: 0.0000019, Val MAE: 1.3483034, Test MAE: 1.3813943\n",
            "Epoch: 080, LR: 0.000387, Loss: 0.0000007, Val MAE: 1.3358126, Test MAE: 1.3813943\n",
            "Epoch: 090, LR: 0.000314, Loss: 0.0000002, Val MAE: 1.3312227, Test MAE: 1.3813943\n",
            "Epoch: 100, LR: 0.000254, Loss: 0.0000001, Val MAE: 1.3298998, Test MAE: 1.3813943\n",
            "\n",
            "Done! Training took 0.08 mins. Best validation MAE: 0.9225230, corresponding test MAE: 1.3813943.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156293\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0025786, Val MAE: 1.2256640, Test MAE: 1.4398673\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0003480, Val MAE: 1.1116774, Test MAE: 1.3418400\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0000231, Val MAE: 0.9004715, Test MAE: 1.4247229\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000330, Val MAE: 0.6672976, Test MAE: 2.1432358\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000056, Val MAE: 0.9755862, Test MAE: 2.1432358\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000040, Val MAE: 1.1592020, Test MAE: 2.1432358\n",
            "Epoch: 070, LR: 0.000656, Loss: 0.0000008, Val MAE: 1.2049950, Test MAE: 2.1432358\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000003, Val MAE: 1.2173808, Test MAE: 2.1432358\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000001, Val MAE: 1.2203695, Test MAE: 2.1432358\n",
            "Epoch: 100, LR: 0.000387, Loss: 0.0000000, Val MAE: 1.2214324, Test MAE: 2.1432358\n",
            "\n",
            "Done! Training took 0.10 mins. Best validation MAE: 0.6672976, corresponding test MAE: 2.1432358.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156293\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0132279, Val MAE: 1.1245840, Test MAE: 1.6082592\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0003796, Val MAE: 1.0813650, Test MAE: 1.3754974\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001187, Val MAE: 0.9932704, Test MAE: 1.2987010\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000378, Val MAE: 1.0762484, Test MAE: 1.4388704\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0000115, Val MAE: 1.2356537, Test MAE: 1.4388704\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000027, Val MAE: 1.3663884, Test MAE: 1.4388704\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000013, Val MAE: 1.4106760, Test MAE: 1.4388704\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000003, Val MAE: 1.4199821, Test MAE: 1.4388704\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000001, Val MAE: 1.4216866, Test MAE: 1.4388704\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000000, Val MAE: 1.4221315, Test MAE: 1.4388704\n",
            "\n",
            "Done! Training took 0.08 mins. Best validation MAE: 0.9271618, corresponding test MAE: 1.4388704.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0306682, Val MAE: 1.1619314, Test MAE: 1.4585456\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0013368, Val MAE: 1.1973723, Test MAE: 1.3367737\n",
            "Epoch: 030, LR: 0.000810, Loss: 0.0002077, Val MAE: 1.1416115, Test MAE: 1.3367737\n",
            "Epoch: 040, LR: 0.000810, Loss: 0.0000918, Val MAE: 0.8031014, Test MAE: 1.6413761\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0000172, Val MAE: 0.5557769, Test MAE: 2.1423824\n",
            "Epoch: 060, LR: 0.000810, Loss: 0.0000044, Val MAE: 0.4519244, Test MAE: 2.4195463\n",
            "Epoch: 070, LR: 0.000810, Loss: 0.0000029, Val MAE: 0.3968342, Test MAE: 2.4972420\n",
            "Epoch: 080, LR: 0.000810, Loss: 0.0000014, Val MAE: 0.3731190, Test MAE: 2.5252256\n",
            "Epoch: 090, LR: 0.000810, Loss: 0.0000004, Val MAE: 0.3654740, Test MAE: 2.5336060\n",
            "Epoch: 100, LR: 0.000810, Loss: 0.0000001, Val MAE: 0.3625747, Test MAE: 2.5361923\n",
            "\n",
            "Done! Training took 0.08 mins. Best validation MAE: 0.3625747, corresponding test MAE: 2.5361923.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0034781, Val MAE: 1.0980293, Test MAE: 1.4067466\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0001504, Val MAE: 0.9425692, Test MAE: 1.3050348\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0000324, Val MAE: 0.7245243, Test MAE: 1.3591436\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000113, Val MAE: 0.5227878, Test MAE: 1.6202370\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000039, Val MAE: 0.5749333, Test MAE: 1.7319345\n",
            "Epoch: 060, LR: 0.000810, Loss: 0.0000010, Val MAE: 0.6977666, Test MAE: 1.7319345\n",
            "Epoch: 070, LR: 0.000656, Loss: 0.0000003, Val MAE: 0.7616376, Test MAE: 1.7319345\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000001, Val MAE: 0.7848452, Test MAE: 1.7319345\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0000000, Val MAE: 0.7932725, Test MAE: 1.7319345\n",
            "Epoch: 100, LR: 0.000387, Loss: 0.0000000, Val MAE: 0.7961392, Test MAE: 1.7319345\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.4761960, corresponding test MAE: 1.7319345.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0078099, Val MAE: 1.1772241, Test MAE: 1.6585344\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0006506, Val MAE: 1.1298474, Test MAE: 1.4437720\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001586, Val MAE: 0.9462328, Test MAE: 1.2591419\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000698, Val MAE: 0.5013147, Test MAE: 1.2752922\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0000093, Val MAE: 0.4384784, Test MAE: 1.5881163\n",
            "Epoch: 060, LR: 0.000900, Loss: 0.0000051, Val MAE: 0.5114349, Test MAE: 1.5881163\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000008, Val MAE: 0.5587654, Test MAE: 1.5881163\n",
            "Epoch: 080, LR: 0.000656, Loss: 0.0000005, Val MAE: 0.5785644, Test MAE: 1.5881163\n",
            "Epoch: 090, LR: 0.000531, Loss: 0.0000002, Val MAE: 0.5856124, Test MAE: 1.5881163\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0000000, Val MAE: 0.5879718, Test MAE: 1.5881163\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.4384784, corresponding test MAE: 1.5881163.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0253079, Val MAE: 1.1678545, Test MAE: 1.5875405\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0003354, Val MAE: 1.0148932, Test MAE: 1.5555683\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0000553, Val MAE: 0.7420146, Test MAE: 1.3772637\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000302, Val MAE: 1.3382598, Test MAE: 1.4221441\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0000076, Val MAE: 1.7985638, Test MAE: 1.4221441\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000036, Val MAE: 2.0245436, Test MAE: 1.4221441\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000008, Val MAE: 2.0696007, Test MAE: 1.4221441\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0000002, Val MAE: 2.0854179, Test MAE: 1.4221441\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000001, Val MAE: 2.0893005, Test MAE: 1.4221441\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000000, Val MAE: 2.0906021, Test MAE: 1.4221441\n",
            "\n",
            "Done! Training took 0.09 mins. Best validation MAE: 0.7345006, corresponding test MAE: 1.4221441.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.4516003, Val MAE: 1.2302133, Test MAE: 2.2492424\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0386800, Val MAE: 0.9536963, Test MAE: 1.6538340\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0009818, Val MAE: 0.9050158, Test MAE: 1.3300610\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0002370, Val MAE: 0.6218295, Test MAE: 1.7529703\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000141, Val MAE: 0.6698795, Test MAE: 1.7529703\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000242, Val MAE: 0.6669999, Test MAE: 1.7529703\n",
            "Epoch: 070, LR: 0.000656, Loss: 0.0000266, Val MAE: 0.6670250, Test MAE: 1.7529703\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000155, Val MAE: 0.6721066, Test MAE: 1.7529703\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000042, Val MAE: 0.6753648, Test MAE: 1.7529703\n",
            "Epoch: 100, LR: 0.000387, Loss: 0.0000002, Val MAE: 0.6759840, Test MAE: 1.7529703\n",
            "\n",
            "Done! Training took 0.10 mins. Best validation MAE: 0.6218295, corresponding test MAE: 1.7529703.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for VirtualEGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "VirtualEGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x VirtualEquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 156037\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0036186, Val MAE: 1.2213345, Test MAE: 1.5097872\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0016301, Val MAE: 0.9943416, Test MAE: 1.3613610\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0002170, Val MAE: 0.7099636, Test MAE: 1.5576516\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000756, Val MAE: 0.6399440, Test MAE: 2.4196783\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000151, Val MAE: 0.7677860, Test MAE: 2.4196783\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000053, Val MAE: 0.7902924, Test MAE: 2.4196783\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0000010, Val MAE: 0.7963207, Test MAE: 2.4196783\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000006, Val MAE: 0.8012474, Test MAE: 2.4196783\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000003, Val MAE: 0.8006392, Test MAE: 2.4196783\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0000001, Val MAE: 0.8016047, Test MAE: 2.4196783\n",
            "\n",
            "Done! Training took 0.09 mins. Best validation MAE: 0.6326766, corresponding test MAE: 2.4196783.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 104069\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0026352, Val MAE: 1.0398794, Test MAE: 1.7983404\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0006171, Val MAE: 0.9264681, Test MAE: 1.4478413\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001680, Val MAE: 0.7443633, Test MAE: 1.7374557\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000302, Val MAE: 0.4664459, Test MAE: 3.1427183\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0000174, Val MAE: 0.4142519, Test MAE: 4.0173889\n",
            "Epoch: 060, LR: 0.001000, Loss: 0.0000067, Val MAE: 0.3981233, Test MAE: 4.3413162\n",
            "Epoch: 070, LR: 0.000810, Loss: 0.0000031, Val MAE: 0.3973700, Test MAE: 4.3413162\n",
            "Epoch: 080, LR: 0.000810, Loss: 0.0000013, Val MAE: 0.3932873, Test MAE: 4.6198143\n",
            "Epoch: 090, LR: 0.000810, Loss: 0.0000004, Val MAE: 0.3914525, Test MAE: 4.6365776\n",
            "Epoch: 100, LR: 0.000810, Loss: 0.0000002, Val MAE: 0.3910168, Test MAE: 4.6419270\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.3909229, corresponding test MAE: 4.6419270.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 104069\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0027464, Val MAE: 1.1888621, Test MAE: 1.4630566\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0003526, Val MAE: 1.1141838, Test MAE: 1.2176119\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001213, Val MAE: 0.8751394, Test MAE: 1.4021451\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000333, Val MAE: 0.6044572, Test MAE: 2.1155060\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000115, Val MAE: 0.7127234, Test MAE: 2.1155060\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000033, Val MAE: 0.7602170, Test MAE: 2.1155060\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0000018, Val MAE: 0.7828441, Test MAE: 2.1155060\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000005, Val MAE: 0.7944784, Test MAE: 2.1155060\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000002, Val MAE: 0.7979412, Test MAE: 2.1155060\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0000001, Val MAE: 0.7992731, Test MAE: 2.1155060\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.5803991, corresponding test MAE: 2.1155060.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 104069\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0590189, Val MAE: 1.1819889, Test MAE: 1.9420881\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0172657, Val MAE: 1.0465094, Test MAE: 1.4722288\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0017998, Val MAE: 0.9708807, Test MAE: 1.3586901\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0006004, Val MAE: 0.9532895, Test MAE: 1.5467676\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0001685, Val MAE: 0.9531063, Test MAE: 1.6307375\n",
            "Epoch: 060, LR: 0.000810, Loss: 0.0000517, Val MAE: 0.9541073, Test MAE: 1.6307375\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000151, Val MAE: 0.9457336, Test MAE: 1.6307375\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0000063, Val MAE: 0.9470510, Test MAE: 1.6307375\n",
            "Epoch: 090, LR: 0.000531, Loss: 0.0000021, Val MAE: 0.9428014, Test MAE: 1.6684479\n",
            "Epoch: 100, LR: 0.000478, Loss: 0.0000005, Val MAE: 0.9431598, Test MAE: 1.6684479\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.9421997, corresponding test MAE: 1.6684479.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 104069\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0044613, Val MAE: 1.1947206, Test MAE: 1.5607813\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0004479, Val MAE: 1.1049708, Test MAE: 1.4479305\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0002345, Val MAE: 0.7728004, Test MAE: 1.3005666\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000735, Val MAE: 0.5117654, Test MAE: 1.4067496\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000213, Val MAE: 0.6411171, Test MAE: 1.4030199\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000061, Val MAE: 0.8147688, Test MAE: 1.4030199\n",
            "Epoch: 070, LR: 0.000656, Loss: 0.0000014, Val MAE: 0.9126880, Test MAE: 1.4030199\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000009, Val MAE: 0.9490742, Test MAE: 1.4030199\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000003, Val MAE: 0.9622211, Test MAE: 1.4030199\n",
            "Epoch: 100, LR: 0.000387, Loss: 0.0000001, Val MAE: 0.9670908, Test MAE: 1.4030199\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.5061962, corresponding test MAE: 1.4030199.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 104069\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0153602, Val MAE: 1.0744512, Test MAE: 1.9378170\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0035698, Val MAE: 0.9253541, Test MAE: 1.5879095\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0005554, Val MAE: 0.4637199, Test MAE: 1.4051321\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0001715, Val MAE: 0.4273196, Test MAE: 1.4710178\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0001025, Val MAE: 0.5307456, Test MAE: 1.4710178\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000304, Val MAE: 0.5532282, Test MAE: 1.4710178\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0000063, Val MAE: 0.5601172, Test MAE: 1.4710178\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000022, Val MAE: 0.5661604, Test MAE: 1.4710178\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000010, Val MAE: 0.5681889, Test MAE: 1.4710178\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0000002, Val MAE: 0.5687792, Test MAE: 1.4710178\n",
            "\n",
            "Done! Training took 0.10 mins. Best validation MAE: 0.3480416, corresponding test MAE: 1.4710178.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 104069\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0137520, Val MAE: 1.1613613, Test MAE: 1.6772703\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0016484, Val MAE: 0.9769466, Test MAE: 1.4796972\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0003724, Val MAE: 0.7217506, Test MAE: 1.3636828\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000416, Val MAE: 0.8771997, Test MAE: 1.3823914\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0000100, Val MAE: 1.3432590, Test MAE: 1.3823914\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000093, Val MAE: 1.7248711, Test MAE: 1.3823914\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000031, Val MAE: 1.8621994, Test MAE: 1.3823914\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000007, Val MAE: 1.9181332, Test MAE: 1.3823914\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000004, Val MAE: 1.9396534, Test MAE: 1.3823914\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000001, Val MAE: 1.9447796, Test MAE: 1.3823914\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.6958618, corresponding test MAE: 1.3823914.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103749\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0023580, Val MAE: 1.0459935, Test MAE: 1.3392053\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0002913, Val MAE: 0.9312843, Test MAE: 1.1141943\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0000572, Val MAE: 1.3133736, Test MAE: 1.3286440\n",
            "Epoch: 040, LR: 0.000810, Loss: 0.0000138, Val MAE: 2.1468334, Test MAE: 1.3286440\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0000032, Val MAE: 3.5258026, Test MAE: 1.3286440\n",
            "Epoch: 060, LR: 0.000590, Loss: 0.0000019, Val MAE: 4.2549427, Test MAE: 1.3286440\n",
            "Epoch: 070, LR: 0.000478, Loss: 0.0000008, Val MAE: 4.5941422, Test MAE: 1.3286440\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0000002, Val MAE: 4.7153152, Test MAE: 1.3286440\n",
            "Epoch: 090, LR: 0.000349, Loss: 0.0000001, Val MAE: 4.7570919, Test MAE: 1.3286440\n",
            "Epoch: 100, LR: 0.000282, Loss: 0.0000000, Val MAE: 4.7712524, Test MAE: 1.3286440\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.8732830, corresponding test MAE: 1.3286440.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103749\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0112434, Val MAE: 1.1077600, Test MAE: 1.6364115\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0028154, Val MAE: 0.8633123, Test MAE: 1.3767052\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0002854, Val MAE: 0.6560807, Test MAE: 1.8947075\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0001638, Val MAE: 0.9287210, Test MAE: 1.8947075\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0000170, Val MAE: 0.7565326, Test MAE: 1.8947075\n",
            "Epoch: 060, LR: 0.000590, Loss: 0.0000168, Val MAE: 0.7926754, Test MAE: 1.8947075\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000051, Val MAE: 0.8990862, Test MAE: 1.8947075\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0000018, Val MAE: 0.9349350, Test MAE: 1.8947075\n",
            "Epoch: 090, LR: 0.000349, Loss: 0.0000005, Val MAE: 0.9485378, Test MAE: 1.8947075\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000001, Val MAE: 0.9529781, Test MAE: 1.8947075\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.6088303, corresponding test MAE: 1.8947075.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103749\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0177849, Val MAE: 0.9907825, Test MAE: 1.5619600\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0009484, Val MAE: 0.8252851, Test MAE: 1.4377428\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0004323, Val MAE: 1.1736893, Test MAE: 1.4377428\n",
            "Epoch: 040, LR: 0.000729, Loss: 0.0000447, Val MAE: 1.6753565, Test MAE: 1.4377428\n",
            "Epoch: 050, LR: 0.000656, Loss: 0.0000290, Val MAE: 2.1885843, Test MAE: 1.4377428\n",
            "Epoch: 060, LR: 0.000531, Loss: 0.0000098, Val MAE: 2.3007442, Test MAE: 1.4377428\n",
            "Epoch: 070, LR: 0.000430, Loss: 0.0000013, Val MAE: 2.3171547, Test MAE: 1.4377428\n",
            "Epoch: 080, LR: 0.000387, Loss: 0.0000013, Val MAE: 2.3321926, Test MAE: 1.4377428\n",
            "Epoch: 090, LR: 0.000314, Loss: 0.0000004, Val MAE: 2.3307316, Test MAE: 1.4377428\n",
            "Epoch: 100, LR: 0.000254, Loss: 0.0000002, Val MAE: 2.3327150, Test MAE: 1.4377428\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.8252851, corresponding test MAE: 1.4377428.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103749\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.2988186, Val MAE: 1.1200638, Test MAE: 1.6660854\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0407663, Val MAE: 1.0612545, Test MAE: 1.2488637\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0010259, Val MAE: 0.9904261, Test MAE: 1.3855265\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0003728, Val MAE: 0.7764679, Test MAE: 1.7926897\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0002332, Val MAE: 0.5858411, Test MAE: 2.0651640\n",
            "Epoch: 060, LR: 0.000810, Loss: 0.0001073, Val MAE: 0.6002550, Test MAE: 2.1212772\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000349, Val MAE: 0.5997334, Test MAE: 2.1212772\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0000072, Val MAE: 0.6043789, Test MAE: 2.1212772\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0000008, Val MAE: 0.6049011, Test MAE: 2.1212772\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0000014, Val MAE: 0.6044707, Test MAE: 2.1212772\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.5723056, corresponding test MAE: 2.1212772.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103749\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0737046, Val MAE: 1.2606724, Test MAE: 2.4136835\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0053060, Val MAE: 0.9338891, Test MAE: 2.0268076\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0010970, Val MAE: 0.8863647, Test MAE: 1.6677151\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0002099, Val MAE: 0.8021486, Test MAE: 1.4230113\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000380, Val MAE: 0.7124138, Test MAE: 1.3791353\n",
            "Epoch: 060, LR: 0.000900, Loss: 0.0000183, Val MAE: 0.5983518, Test MAE: 1.3660442\n",
            "Epoch: 070, LR: 0.000900, Loss: 0.0000050, Val MAE: 0.5429084, Test MAE: 1.3657272\n",
            "Epoch: 080, LR: 0.000900, Loss: 0.0000010, Val MAE: 0.5203090, Test MAE: 1.3589334\n",
            "Epoch: 090, LR: 0.000900, Loss: 0.0000008, Val MAE: 0.5116020, Test MAE: 1.3565124\n",
            "Epoch: 100, LR: 0.000900, Loss: 0.0000003, Val MAE: 0.5092601, Test MAE: 1.3551052\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.5092601, corresponding test MAE: 1.3551052.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for EGNN, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "EGNN(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x EquivariantMPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103749\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0013363, Val MAE: 1.1088732, Test MAE: 1.4610044\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0003811, Val MAE: 1.0775561, Test MAE: 1.2627228\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0002790, Val MAE: 0.7528656, Test MAE: 1.4175760\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000783, Val MAE: 0.5901503, Test MAE: 1.9300407\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0000199, Val MAE: 0.4800655, Test MAE: 2.3132515\n",
            "Epoch: 060, LR: 0.001000, Loss: 0.0000060, Val MAE: 0.4410735, Test MAE: 2.4208216\n",
            "Epoch: 070, LR: 0.001000, Loss: 0.0000710, Val MAE: 0.4392621, Test MAE: 2.4472862\n",
            "Epoch: 080, LR: 0.000900, Loss: 0.0000024, Val MAE: 0.4358014, Test MAE: 2.4670021\n",
            "Epoch: 090, LR: 0.000900, Loss: 0.0000134, Val MAE: 0.4294282, Test MAE: 2.4676407\n",
            "Epoch: 100, LR: 0.000900, Loss: 0.0000017, Val MAE: 0.4253683, Test MAE: 2.4653225\n",
            "\n",
            "Done! Training took 0.07 mins. Best validation MAE: 0.4253683, corresponding test MAE: 2.4653225.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103553\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0058901, Val MAE: 1.1562303, Test MAE: 1.4776997\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0017714, Val MAE: 1.1016045, Test MAE: 1.2705776\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0001535, Val MAE: 1.0234524, Test MAE: 1.1883251\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000599, Val MAE: 0.7912100, Test MAE: 1.2370881\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0000584, Val MAE: 0.6482033, Test MAE: 1.3789549\n",
            "Epoch: 060, LR: 0.000900, Loss: 0.0000154, Val MAE: 0.6378770, Test MAE: 1.4303789\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000012, Val MAE: 0.6471757, Test MAE: 1.4303789\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0000014, Val MAE: 0.6523234, Test MAE: 1.4303789\n",
            "Epoch: 090, LR: 0.000531, Loss: 0.0000005, Val MAE: 0.6524253, Test MAE: 1.4303789\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0000002, Val MAE: 0.6536379, Test MAE: 1.4303789\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.6295060, corresponding test MAE: 1.4303789.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103553\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.000900, Loss: 0.0093956, Val MAE: 1.1770402, Test MAE: 1.5805183\n",
            "Epoch: 020, LR: 0.000810, Loss: 0.0034709, Val MAE: 1.2542784, Test MAE: 1.5805183\n",
            "Epoch: 030, LR: 0.000729, Loss: 0.0002805, Val MAE: 1.0566651, Test MAE: 1.1826866\n",
            "Epoch: 040, LR: 0.000729, Loss: 0.0000801, Val MAE: 0.7414127, Test MAE: 1.3242659\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0000354, Val MAE: 0.5421827, Test MAE: 1.5099476\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000139, Val MAE: 0.5850299, Test MAE: 1.5099476\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000062, Val MAE: 0.6128329, Test MAE: 1.5099476\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0000016, Val MAE: 0.6207482, Test MAE: 1.5099476\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000006, Val MAE: 0.6241626, Test MAE: 1.5099476\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000001, Val MAE: 0.6249572, Test MAE: 1.5099476\n",
            "\n",
            "Done! Training took 0.04 mins. Best validation MAE: 0.5377069, corresponding test MAE: 1.5099476.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103553\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0064236, Val MAE: 1.1830757, Test MAE: 1.5538279\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0011253, Val MAE: 0.9801815, Test MAE: 1.3420633\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0003251, Val MAE: 0.5851375, Test MAE: 1.4241997\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000791, Val MAE: 0.4509424, Test MAE: 1.6470814\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0000731, Val MAE: 0.4896963, Test MAE: 1.6470814\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000166, Val MAE: 0.5041505, Test MAE: 1.6470814\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0000041, Val MAE: 0.5060079, Test MAE: 1.6470814\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000011, Val MAE: 0.5063726, Test MAE: 1.6470814\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000012, Val MAE: 0.5058166, Test MAE: 1.6470814\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0000006, Val MAE: 0.5053675, Test MAE: 1.6470814\n",
            "\n",
            "Done! Training took 0.04 mins. Best validation MAE: 0.4347326, corresponding test MAE: 1.6470814.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103553\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0906658, Val MAE: 1.0865839, Test MAE: 1.8161522\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0286084, Val MAE: 0.9267819, Test MAE: 1.2746898\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0034062, Val MAE: 0.6341138, Test MAE: 1.6565296\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0010360, Val MAE: 0.8711327, Test MAE: 1.9113491\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0002705, Val MAE: 1.2032539, Test MAE: 1.9113491\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000799, Val MAE: 1.3141211, Test MAE: 1.9113491\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000192, Val MAE: 1.3643430, Test MAE: 1.9113491\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000046, Val MAE: 1.3764315, Test MAE: 1.9113491\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000019, Val MAE: 1.3829857, Test MAE: 1.9113491\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000006, Val MAE: 1.3828519, Test MAE: 1.9113491\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.6259104, corresponding test MAE: 1.9113491.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103553\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0028080, Val MAE: 1.1858931, Test MAE: 1.5568249\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0006590, Val MAE: 0.9496979, Test MAE: 1.2167688\n",
            "Epoch: 030, LR: 0.000900, Loss: 0.0001438, Val MAE: 1.2896881, Test MAE: 1.2167688\n",
            "Epoch: 040, LR: 0.000729, Loss: 0.0000481, Val MAE: 1.5595361, Test MAE: 1.2167688\n",
            "Epoch: 050, LR: 0.000656, Loss: 0.0000090, Val MAE: 1.4813108, Test MAE: 1.2167688\n",
            "Epoch: 060, LR: 0.000531, Loss: 0.0000032, Val MAE: 1.4706512, Test MAE: 1.2167688\n",
            "Epoch: 070, LR: 0.000430, Loss: 0.0000005, Val MAE: 1.4654518, Test MAE: 1.2167688\n",
            "Epoch: 080, LR: 0.000387, Loss: 0.0000002, Val MAE: 1.4602603, Test MAE: 1.2167688\n",
            "Epoch: 090, LR: 0.000314, Loss: 0.0000001, Val MAE: 1.4570091, Test MAE: 1.2167688\n",
            "Epoch: 100, LR: 0.000254, Loss: 0.0000000, Val MAE: 1.4559530, Test MAE: 1.2167688\n",
            "\n",
            "Done! Training took 0.08 mins. Best validation MAE: 0.9496979, corresponding test MAE: 1.2167688.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=12, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103553\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0031978, Val MAE: 1.1285256, Test MAE: 1.5237110\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0009933, Val MAE: 0.9924408, Test MAE: 1.3124075\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0002768, Val MAE: 0.6523026, Test MAE: 1.5493719\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000375, Val MAE: 0.7091738, Test MAE: 1.5759637\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0000234, Val MAE: 0.8216560, Test MAE: 1.5759637\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000023, Val MAE: 0.7894650, Test MAE: 1.5759637\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000013, Val MAE: 0.7727575, Test MAE: 1.5759637\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0000008, Val MAE: 0.7683212, Test MAE: 1.5759637\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000002, Val MAE: 0.7671847, Test MAE: 1.5759637\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000001, Val MAE: 0.7665708, Test MAE: 1.5759637\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.6228473, corresponding test MAE: 1.5759637.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103233\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0038820, Val MAE: 1.0519711, Test MAE: 1.5546545\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0011259, Val MAE: 0.6890031, Test MAE: 1.3791960\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0003362, Val MAE: 0.6737526, Test MAE: 1.5229134\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0001335, Val MAE: 0.7843964, Test MAE: 1.8644609\n",
            "Epoch: 050, LR: 0.000810, Loss: 0.0000384, Val MAE: 1.2515876, Test MAE: 1.8644609\n",
            "Epoch: 060, LR: 0.000729, Loss: 0.0000071, Val MAE: 1.3811256, Test MAE: 1.8644609\n",
            "Epoch: 070, LR: 0.000590, Loss: 0.0000019, Val MAE: 1.3912357, Test MAE: 1.8644609\n",
            "Epoch: 080, LR: 0.000478, Loss: 0.0000012, Val MAE: 1.3903541, Test MAE: 1.8644609\n",
            "Epoch: 090, LR: 0.000430, Loss: 0.0000005, Val MAE: 1.3897593, Test MAE: 1.8644609\n",
            "Epoch: 100, LR: 0.000349, Loss: 0.0000001, Val MAE: 1.3888414, Test MAE: 1.8644609\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.5410406, corresponding test MAE: 1.8644609.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103233\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.000900, Loss: 0.0023051, Val MAE: 1.1792118, Test MAE: 1.3667818\n",
            "Epoch: 020, LR: 0.000900, Loss: 0.0005253, Val MAE: 1.1493571, Test MAE: 1.1242931\n",
            "Epoch: 030, LR: 0.000810, Loss: 0.0001376, Val MAE: 0.8284076, Test MAE: 2.5471127\n",
            "Epoch: 040, LR: 0.000729, Loss: 0.0000461, Val MAE: 3.5783939, Test MAE: 2.5471127\n",
            "Epoch: 050, LR: 0.000590, Loss: 0.0000169, Val MAE: 7.0687332, Test MAE: 2.5471127\n",
            "Epoch: 060, LR: 0.000531, Loss: 0.0000064, Val MAE: 8.8972603, Test MAE: 2.5471127\n",
            "Epoch: 070, LR: 0.000430, Loss: 0.0000011, Val MAE: 9.6286209, Test MAE: 2.5471127\n",
            "Epoch: 080, LR: 0.000349, Loss: 0.0000005, Val MAE: 9.8971886, Test MAE: 2.5471127\n",
            "Epoch: 090, LR: 0.000314, Loss: 0.0000001, Val MAE: 9.9957581, Test MAE: 2.5471127\n",
            "Epoch: 100, LR: 0.000254, Loss: 0.0000000, Val MAE: 10.0308952, Test MAE: 2.5471127\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.8284076, corresponding test MAE: 2.5471127.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103233\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0049904, Val MAE: 1.0548059, Test MAE: 1.3381407\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0002328, Val MAE: 0.9260231, Test MAE: 1.1719169\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001610, Val MAE: 0.7530972, Test MAE: 1.6157276\n",
            "Epoch: 040, LR: 0.000900, Loss: 0.0000438, Val MAE: 0.7750143, Test MAE: 1.5912709\n",
            "Epoch: 050, LR: 0.000729, Loss: 0.0000144, Val MAE: 0.9208461, Test MAE: 1.5912709\n",
            "Epoch: 060, LR: 0.000656, Loss: 0.0000011, Val MAE: 1.2514700, Test MAE: 1.5912709\n",
            "Epoch: 070, LR: 0.000531, Loss: 0.0000009, Val MAE: 1.4176382, Test MAE: 1.5912709\n",
            "Epoch: 080, LR: 0.000430, Loss: 0.0000004, Val MAE: 1.4458450, Test MAE: 1.5912709\n",
            "Epoch: 090, LR: 0.000387, Loss: 0.0000002, Val MAE: 1.4551374, Test MAE: 1.5912709\n",
            "Epoch: 100, LR: 0.000314, Loss: 0.0000000, Val MAE: 1.4587687, Test MAE: 1.5912709\n",
            "\n",
            "Done! Training took 0.06 mins. Best validation MAE: 0.5980742, corresponding test MAE: 1.5912709.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103233\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0093224, Val MAE: 1.1041636, Test MAE: 1.6200190\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0005984, Val MAE: 1.0178226, Test MAE: 1.2687319\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0002094, Val MAE: 0.8541658, Test MAE: 1.3394444\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000402, Val MAE: 0.4763713, Test MAE: 1.6713547\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0000188, Val MAE: 0.3661765, Test MAE: 1.8461369\n",
            "Epoch: 060, LR: 0.000810, Loss: 0.0000051, Val MAE: 0.4346753, Test MAE: 1.8461369\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000017, Val MAE: 0.4862378, Test MAE: 1.8461369\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0000004, Val MAE: 0.5044909, Test MAE: 1.8461369\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0000002, Val MAE: 0.5104462, Test MAE: 1.8461369\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0000001, Val MAE: 0.5125351, Test MAE: 1.8461369\n",
            "\n",
            "Done! Training took 0.05 mins. Best validation MAE: 0.3548065, corresponding test MAE: 1.8461369.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103233\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.0252475, Val MAE: 1.0734318, Test MAE: 1.5737860\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0004688, Val MAE: 0.8567112, Test MAE: 1.5721730\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0001671, Val MAE: 0.6915357, Test MAE: 1.6822739\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0000378, Val MAE: 0.5111245, Test MAE: 1.8342598\n",
            "Epoch: 050, LR: 0.001000, Loss: 0.0000210, Val MAE: 0.3829637, Test MAE: 1.9008324\n",
            "Epoch: 060, LR: 0.000900, Loss: 0.0000044, Val MAE: 0.4239347, Test MAE: 1.9008324\n",
            "Epoch: 070, LR: 0.000729, Loss: 0.0000014, Val MAE: 0.4405487, Test MAE: 1.9008324\n",
            "Epoch: 080, LR: 0.000590, Loss: 0.0000007, Val MAE: 0.4465479, Test MAE: 1.9008324\n",
            "Epoch: 090, LR: 0.000531, Loss: 0.0000001, Val MAE: 0.4489851, Test MAE: 1.9008324\n",
            "Epoch: 100, LR: 0.000430, Loss: 0.0000001, Val MAE: 0.4497388, Test MAE: 1.9008324\n",
            "\n",
            "Done! Training took 0.04 mins. Best validation MAE: 0.3805406, corresponding test MAE: 1.9008324.\n",
            "Created dataset splits with 10 training, 10 validation, 10 test samples.\n",
            "Running experiment for MPNNModel, training on 10 samples for 100 epochs.\n",
            "\n",
            "Model architecture:\n",
            "MPNNModel(\n",
            "  (lin_in): Linear(in_features=11, out_features=64, bias=True)\n",
            "  (convs): ModuleList(\n",
            "    (0-3): 4 x MPNNLayer(emb_dim=64, aggr=add)\n",
            "  )\n",
            "  (lin_pred): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Total parameters: 103233\n",
            "\n",
            "Start training:\n",
            "Epoch: 010, LR: 0.001000, Loss: 0.1526238, Val MAE: 1.2288651, Test MAE: 1.6361073\n",
            "Epoch: 020, LR: 0.001000, Loss: 0.0159103, Val MAE: 1.0609910, Test MAE: 1.3741173\n",
            "Epoch: 030, LR: 0.001000, Loss: 0.0016647, Val MAE: 0.7859342, Test MAE: 1.4440209\n",
            "Epoch: 040, LR: 0.001000, Loss: 0.0006589, Val MAE: 0.5540049, Test MAE: 1.6154106\n",
            "Epoch: 050, LR: 0.000900, Loss: 0.0002662, Val MAE: 0.5609723, Test MAE: 1.6527334\n",
            "Epoch: 060, LR: 0.000810, Loss: 0.0000740, Val MAE: 0.5882067, Test MAE: 1.6527334\n",
            "Epoch: 070, LR: 0.000656, Loss: 0.0000205, Val MAE: 0.5942706, Test MAE: 1.6527334\n",
            "Epoch: 080, LR: 0.000531, Loss: 0.0000081, Val MAE: 0.5963820, Test MAE: 1.6527334\n",
            "Epoch: 090, LR: 0.000478, Loss: 0.0000010, Val MAE: 0.5972385, Test MAE: 1.6527334\n",
            "Epoch: 100, LR: 0.000387, Loss: 0.0000004, Val MAE: 0.5983091, Test MAE: 1.6527334\n",
            "\n",
            "Done! Training took 0.04 mins. Best validation MAE: 0.5264208, corresponding test MAE: 1.6527334.\n"
          ]
        }
      ]
    }
  ]
}